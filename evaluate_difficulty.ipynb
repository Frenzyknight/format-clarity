{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen3-32B Difficulty Evaluation\n",
    "\n",
    "Evaluates the Qwen3-32B thinking model on training data to identify \"tough\" questions (ones the model gets wrong) and outputs a CSV with difficulty classifications.\n",
    "\n",
    "**Output CSV columns:**\n",
    "- `idx`: Row index from training data\n",
    "- `prediction`: Model's predicted label\n",
    "- `golden_label`: Ground truth from training data\n",
    "- `difficulty`: \"hard\" or \"easy\"\n",
    "- `raw_output`: Full model response (including thinking)\n",
    "- `question_snippet`: First 200 chars of the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "%pip install transformers accelerate pandas tqdm scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Suppress attention mask warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*attention_mask.*\")\n",
    "\n",
    "# Check available GPUs\n",
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    print(f\"  GPU {i}: {props.name} ({props.total_memory / 1024**3:.1f} GB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "# Change these as needed\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-32B\"          # Qwen3-32B with thinking mode\n",
    "DATA_PATH = \"data/train/train.parquet\" # Training data\n",
    "OUTPUT_PATH = \"difficulty_analysis.csv\" # Output file\n",
    "\n",
    "# Generation parameters for thinking mode\n",
    "MAX_NEW_TOKENS = 1024   # Longer for thinking mode output\n",
    "TEMPERATURE = 0.6       # Qwen3 recommended for thinking\n",
    "TOP_P = 0.95\n",
    "TOP_K = 20\n",
    "MAX_INPUT_LENGTH = 6144  # Increased for few-shot examples\n",
    "\n",
    "# Enable thinking mode (set to False for standard generation)\n",
    "ENABLE_THINKING = True\n",
    "\n",
    "# Set to a number for testing (e.g., 10), None for full run\n",
    "LIMIT = None\n",
    "\n",
    "# Valid labels for classification\n",
    "VALID_LABELS = {\"Clear Reply\", \"Clear Non-Reply\", \"Ambivalent\"}\n",
    "\n",
    "# Few-shot examples for classification\n",
    "FEW_SHOT_EXAMPLES = \"\"\"\n",
    "Here are examples for each classification category:\n",
    "\n",
    "### Example 1: Clear Reply ###\n",
    "Question: Do you have your own views about PR at Westminster don't you?\n",
    "Answer: I do.\n",
    "Label: Clear Reply\n",
    "Explanation: The answer directly gives the info requested.\n",
    "\n",
    "### Example 2: Ambivalent ###\n",
    "Question: Are you going to watch television?\n",
    "Answer: What else is there to do?\n",
    "Label: Ambivalent\n",
    "Explanation: They suggest planning to watch TV, despite not explicitly stating it.\n",
    "\n",
    "### Example 3: Ambivalent ###\n",
    "Question: Do you like my new dress?\n",
    "Answer: We are late.\n",
    "Label: Ambivalent\n",
    "Explanation: Does not even acknowledge the question and goes straight to another topic.\n",
    "\n",
    "### Example 4: Ambivalent ###\n",
    "Question: Did you eat the last piece of pie?\n",
    "Answer: I have to admit that this was a great recipe, I always like it when there are chocolate chips in the dough.\n",
    "Label: Ambivalent\n",
    "Explanation: Acknowledges the question but goes on a tangent about the chips, without answering.\n",
    "\n",
    "### Example 5: Ambivalent ###\n",
    "Question: Did you enjoy the film?\n",
    "Answer: The directing was great.\n",
    "Label: Ambivalent\n",
    "Explanation: Directing is only part of what constitutes a film.\n",
    "\n",
    "### Example 6: Ambivalent ###\n",
    "Question: What's your favorite film?\n",
    "Answer: Fight Club, Filth, and Hereditary.\n",
    "Label: Ambivalent\n",
    "Explanation: The reply gives three movies instead of one, which makes the desired information unclear.\n",
    "\n",
    "### Example 7: Clear Non-Reply ###\n",
    "Question: The hypothesis I was discussing, wouldn't you regard that as a defeat?\n",
    "Answer: I am not going to prophesy what will happen.\n",
    "Label: Clear Non-Reply\n",
    "Explanation: Directly stating they won't answer.\n",
    "\n",
    "### Example 8: Clear Non-Reply ###\n",
    "Question: On what precise date did the government order the refit of the HMAS Kanimbla in preparation for its forward deployment to a possible war against Iraq?\n",
    "Answer: I do not know that date. I will find out and let the House know.\n",
    "Label: Clear Non-Reply\n",
    "Explanation: Claims/admits they don't have the information.\n",
    "\n",
    "### Example 9: Clear Non-Reply ###\n",
    "Question: Was it your decision to release the fund?\n",
    "Answer: You mean the public fund?\n",
    "Label: Clear Non-Reply\n",
    "Explanation: Gives no data, asks for clarification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== HELPER FUNCTIONS ====================\n",
    "\n",
    "def extract_label(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract classification label from thinking model response.\n",
    "    \n",
    "    Handles both thinking mode output (with <think>...</think> blocks)\n",
    "    and standard output formats.\n",
    "    \"\"\"\n",
    "    # Remove thinking block if present to get the final answer\n",
    "    response_clean = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)\n",
    "    response_clean = response_clean.strip()\n",
    "    \n",
    "    # Look for explicit LABEL: pattern (common in COT responses)\n",
    "    match = re.search(\n",
    "        r'LABEL:\\s*(Clear Reply|Clear Non-Reply|Ambivalent)',\n",
    "        response_clean,\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    if match:\n",
    "        label = match.group(1).strip()\n",
    "        return normalize_label(label)\n",
    "    \n",
    "    # Check last 150 chars for the final answer\n",
    "    response_lower = response_clean.lower()\n",
    "    last_part = response_lower[-150:] if len(response_lower) > 150 else response_lower\n",
    "    \n",
    "    # Priority order: more specific labels first\n",
    "    if \"clear non-reply\" in last_part or \"non-reply\" in last_part:\n",
    "        return \"Clear Non-Reply\"\n",
    "    elif \"clear reply\" in last_part:\n",
    "        return \"Clear Reply\"\n",
    "    elif \"ambivalent\" in last_part:\n",
    "        return \"Ambivalent\"\n",
    "    \n",
    "    # Check entire response if not found in last part\n",
    "    if \"clear non-reply\" in response_lower or \"non-reply\" in response_lower:\n",
    "        return \"Clear Non-Reply\"\n",
    "    elif \"clear reply\" in response_lower:\n",
    "        return \"Clear Reply\"\n",
    "    elif \"ambivalent\" in response_lower:\n",
    "        return \"Ambivalent\"\n",
    "    \n",
    "    return \"PARSE_ERROR\"\n",
    "\n",
    "\n",
    "def normalize_label(label: str) -> str:\n",
    "    \"\"\"Normalize label to standard format.\"\"\"\n",
    "    label_lower = label.lower().strip()\n",
    "    if \"non-reply\" in label_lower or \"non reply\" in label_lower:\n",
    "        return \"Clear Non-Reply\"\n",
    "    elif \"clear reply\" in label_lower:\n",
    "        return \"Clear Reply\"\n",
    "    elif \"ambivalent\" in label_lower:\n",
    "        return \"Ambivalent\"\n",
    "    return label\n",
    "\n",
    "\n",
    "def extract_ground_truth(conversations: list) -> str:\n",
    "    \"\"\"Extract ground truth label from the assistant message in conversations.\"\"\"\n",
    "    for msg in conversations:\n",
    "        if msg.get(\"role\") == \"assistant\":\n",
    "            return normalize_label(msg.get(\"content\", \"\").strip())\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "\n",
    "def extract_question_snippet(conversations: list, max_len: int = 200) -> str:\n",
    "    \"\"\"Extract a snippet of the user's question for reference.\"\"\"\n",
    "    for msg in conversations:\n",
    "        if msg.get(\"role\") == \"user\":\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            # Find the \"Specific Question\" section if present\n",
    "            match = re.search(r'### Specific Question[^#]*###\\s*(.+?)(?:\\n|$)', content, re.DOTALL)\n",
    "            if match:\n",
    "                snippet = match.group(1).strip()\n",
    "            else:\n",
    "                snippet = content\n",
    "            \n",
    "            if len(snippet) > max_len:\n",
    "                snippet = snippet[:max_len] + \"...\"\n",
    "            return snippet\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def prepare_messages_for_inference(conversations: list, few_shot: str = None) -> list:\n",
    "    \"\"\"\n",
    "    Prepare conversation messages for inference.\n",
    "    Removes the assistant message (ground truth) and optionally adds few-shot examples.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    for msg in conversations:\n",
    "        if msg.get(\"role\") == \"assistant\":\n",
    "            continue  # Skip ground truth\n",
    "        elif msg.get(\"role\") == \"system\" and few_shot:\n",
    "            # Append few-shot examples to system message\n",
    "            messages.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": msg[\"content\"] + \"\\n\\n\" + few_shot\n",
    "            })\n",
    "        else:\n",
    "            messages.append({\n",
    "                \"role\": msg[\"role\"],\n",
    "                \"content\": msg[\"content\"]\n",
    "            })\n",
    "    return messages\n",
    "\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== LOAD MODEL (2 GPUs) ====================\n",
    "print(\"=\" * 60)\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"Distributing across available GPUs with FP16...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"\\nLoading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model distributed across GPUs\n",
    "print(\"Loading model with device_map='auto' (distributes across all GPUs)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"\\nModel loaded successfully!\")\n",
    "if hasattr(model, 'hf_device_map'):\n",
    "    devices = set(model.hf_device_map.values())\n",
    "    print(f\"Model distributed across devices: {devices}\")\n",
    "    device_counts = {}\n",
    "    for layer, device in model.hf_device_map.items():\n",
    "        device_counts[device] = device_counts.get(device, 0) + 1\n",
    "    for device, count in sorted(device_counts.items()):\n",
    "        print(f\"  Device {device}: {count} modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== LOAD TRAINING DATA ====================\n",
    "print(f\"Loading training data from: {DATA_PATH}\")\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "print(f\"Loaded {len(df)} training examples\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "if LIMIT:\n",
    "    print(f\"\\nLimiting to first {LIMIT} examples (for testing)\")\n",
    "    df = df.head(LIMIT)\n",
    "\n",
    "# Preview sample\n",
    "print(\"\\nSample ground truth labels:\")\n",
    "sample_labels = [extract_ground_truth(row[\"conversations\"]) for _, row in df.head(5).iterrows()]\n",
    "for i, label in enumerate(sample_labels):\n",
    "    print(f\"  {i+1}. {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== RUN INFERENCE ====================\n",
    "results = []\n",
    "total = len(df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Starting inference on {total} examples\")\n",
    "print(f\"Thinking mode: {'enabled' if ENABLE_THINKING else 'disabled'}\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=total, desc=\"Evaluating\"):\n",
    "    conversations = row[\"conversations\"]\n",
    "    \n",
    "    # Convert numpy array to list if needed\n",
    "    if hasattr(conversations, 'tolist'):\n",
    "        conversations = conversations.tolist()\n",
    "    \n",
    "    # Extract ground truth before removing assistant message\n",
    "    golden_label = extract_ground_truth(conversations)\n",
    "    question_snippet = extract_question_snippet(conversations)\n",
    "    \n",
    "    # Prepare messages for inference (remove assistant message, add few-shot examples)\n",
    "    messages = prepare_messages_for_inference(conversations, few_shot=FEW_SHOT_EXAMPLES)\n",
    "    \n",
    "    # Apply chat template\n",
    "    try:\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"[{idx + 1}/{total}] Template error: {e}\")\n",
    "        results.append({\n",
    "            \"idx\": idx,\n",
    "            \"prediction\": \"ERROR\",\n",
    "            \"golden_label\": golden_label,\n",
    "            \"difficulty\": \"unknown\",\n",
    "            \"raw_output\": f\"Template error: {e}\",\n",
    "            \"question_snippet\": question_snippet,\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Tokenize\n",
    "    encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = encoded[\"input_ids\"]\n",
    "    input_len = inputs.shape[-1]\n",
    "    \n",
    "    # Skip if too long\n",
    "    if input_len > MAX_INPUT_LENGTH:\n",
    "        tqdm.write(f\"[{idx + 1}/{total}] SKIPPED (input too long: {input_len} tokens)\")\n",
    "        results.append({\n",
    "            \"idx\": idx,\n",
    "            \"prediction\": \"SKIPPED\",\n",
    "            \"golden_label\": golden_label,\n",
    "            \"difficulty\": \"unknown\",\n",
    "            \"raw_output\": f\"Input too long: {input_len} tokens\",\n",
    "            \"question_snippet\": question_snippet,\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Move to model's device\n",
    "        inputs = inputs.to(model.device)\n",
    "        attention_mask = torch.ones_like(inputs)\n",
    "        \n",
    "        # Generate with thinking-optimized parameters\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                temperature=TEMPERATURE,\n",
    "                top_p=TOP_P,\n",
    "                top_k=TOP_K,\n",
    "                do_sample=True,\n",
    "                use_cache=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode only the new tokens (preserve <think> tags)\n",
    "        full_response = tokenizer.decode(\n",
    "            outputs[0, input_len:],\n",
    "            skip_special_tokens=False\n",
    "        ).strip()\n",
    "        \n",
    "        # Extract prediction label\n",
    "        prediction = extract_label(full_response)\n",
    "        \n",
    "        # Determine difficulty\n",
    "        if prediction in VALID_LABELS and golden_label in VALID_LABELS:\n",
    "            difficulty = \"easy\" if prediction == golden_label else \"hard\"\n",
    "        else:\n",
    "            difficulty = \"unknown\"\n",
    "        \n",
    "        results.append({\n",
    "            \"idx\": idx,\n",
    "            \"prediction\": prediction,\n",
    "            \"golden_label\": golden_label,\n",
    "            \"difficulty\": difficulty,\n",
    "            \"raw_output\": full_response,\n",
    "            \"question_snippet\": question_snippet,\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"[{idx + 1}/{total}] ERROR: {e}\")\n",
    "        results.append({\n",
    "            \"idx\": idx,\n",
    "            \"prediction\": \"ERROR\",\n",
    "            \"golden_label\": golden_label,\n",
    "            \"difficulty\": \"unknown\",\n",
    "            \"raw_output\": str(e),\n",
    "            \"question_snippet\": question_snippet,\n",
    "        })\n",
    "    \n",
    "    # Clear CUDA cache periodically\n",
    "    if (idx + 1) % 25 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nInference complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CREATE RESULTS DATAFRAME ====================\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Saved results to: {OUTPUT_PATH}\")\n",
    "print(f\"DataFrame shape: {results_df.shape}\")\n",
    "\n",
    "# Preview results\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== DIFFICULTY ANALYSIS ====================\n",
    "print(\"=\" * 60)\n",
    "print(\"DIFFICULTY ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total = len(results_df)\n",
    "\n",
    "# Count by difficulty\n",
    "difficulty_counts = results_df[\"difficulty\"].value_counts()\n",
    "\n",
    "print(f\"\\nTotal examples: {total}\")\n",
    "print(\"\\nDifficulty Distribution:\")\n",
    "for diff in [\"easy\", \"hard\", \"unknown\"]:\n",
    "    count = difficulty_counts.get(diff, 0)\n",
    "    pct = count / total * 100 if total > 0 else 0\n",
    "    print(f\"  {diff:10}: {count:5} ({pct:5.1f}%)\")\n",
    "\n",
    "# Filter to valid predictions only\n",
    "valid_mask = results_df[\"difficulty\"].isin([\"easy\", \"hard\"])\n",
    "valid_df = results_df[valid_mask]\n",
    "\n",
    "if len(valid_df) > 0:\n",
    "    # Accuracy\n",
    "    accuracy = (valid_df[\"difficulty\"] == \"easy\").sum() / len(valid_df)\n",
    "    print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Breakdown by ground truth label\n",
    "    print(\"\\nDifficulty by Ground Truth Label:\")\n",
    "    for label in VALID_LABELS:\n",
    "        label_df = valid_df[valid_df[\"golden_label\"] == label]\n",
    "        if len(label_df) > 0:\n",
    "            hard_count = (label_df[\"difficulty\"] == \"hard\").sum()\n",
    "            hard_pct = hard_count / len(label_df) * 100\n",
    "            print(f\"  {label:20}: {hard_count:4}/{len(label_df):4} hard ({hard_pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CLASSIFICATION METRICS ====================\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Filter to valid predictions only\n",
    "y_true = results_df[\"golden_label\"]\n",
    "y_pred = results_df[\"prediction\"]\n",
    "\n",
    "mask = y_true.isin(VALID_LABELS) & y_pred.isin(VALID_LABELS)\n",
    "y_true_filtered = y_true[mask]\n",
    "y_pred_filtered = y_pred[mask]\n",
    "\n",
    "print(f\"Valid predictions: {mask.sum()} / {len(mask)}\")\n",
    "print(f\"Filtered out: {(~mask).sum()} (PARSE_ERROR, SKIPPED, ERROR)\")\n",
    "\n",
    "if len(y_true_filtered) > 0:\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
    "    \n",
    "    # Precision, Recall, F1 (Macro)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true_filtered,\n",
    "        y_pred_filtered,\n",
    "        average=\"macro\",\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Weighted F1\n",
    "    f1_weighted = f1_score(\n",
    "        y_true_filtered,\n",
    "        y_pred_filtered,\n",
    "        average=\"weighted\",\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CLASSIFICATION METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nAccuracy        : {accuracy:.4f}\")\n",
    "    print(f\"Precision (Mac) : {precision_macro:.4f}\")\n",
    "    print(f\"Recall (Mac)    : {recall_macro:.4f}\")\n",
    "    print(f\"F1 (Macro)      : {f1_macro:.4f}\")\n",
    "    print(f\"F1 (Weighted)   : {f1_weighted:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(classification_report(\n",
    "        y_true_filtered,\n",
    "        y_pred_filtered,\n",
    "        labels=list(VALID_LABELS),\n",
    "        zero_division=0\n",
    "    ))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    labels = [\"Clear Reply\", \"Clear Non-Reply\", \"Ambivalent\"]\n",
    "    cm = confusion_matrix(y_true_filtered, y_pred_filtered, labels=labels)\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'':20} {'Pred CR':>12} {'Pred CNR':>12} {'Pred Amb':>12}\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, true_label in enumerate(labels):\n",
    "        row = f\"{true_label:20}\"\n",
    "        for j in range(3):\n",
    "            row += f\" {cm[i][j]:>12}\"\n",
    "        print(row)\n",
    "    print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"No valid predictions to compute metrics!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== PROCESSING ISSUES ====================\n",
    "parse_errors = (results_df[\"prediction\"] == \"PARSE_ERROR\").sum()\n",
    "skipped = (results_df[\"prediction\"] == \"SKIPPED\").sum()\n",
    "errors = (results_df[\"prediction\"] == \"ERROR\").sum()\n",
    "\n",
    "print(\"Processing Issues:\")\n",
    "print(f\"  Parse errors: {parse_errors}\")\n",
    "print(f\"  Skipped:      {skipped}\")\n",
    "print(f\"  Errors:       {errors}\")\n",
    "\n",
    "# Show breakdown of predictions\n",
    "print(\"\\nPrediction Distribution:\")\n",
    "print(results_df[\"prediction\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== HARD QUESTIONS ANALYSIS ====================\n",
    "hard_df = results_df[results_df[\"difficulty\"] == \"hard\"]\n",
    "\n",
    "print(f\"Total HARD questions: {len(hard_df)}\")\n",
    "print(\"\\nSample hard questions (first 10):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, row in hard_df.head(10).iterrows():\n",
    "    print(f\"\\nIdx: {row['idx']}\")\n",
    "    print(f\"Golden Label: {row['golden_label']}\")\n",
    "    print(f\"Prediction:   {row['prediction']}\")\n",
    "    print(f\"Question:     {row['question_snippet'][:100]}...\")\n",
    "    raw_preview = row['raw_output'][:150] if len(row['raw_output']) > 150 else row['raw_output']\n",
    "    print(f\"Raw Output:   {raw_preview}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== EXPORT HARD QUESTIONS ====================\n",
    "hard_output_path = OUTPUT_PATH.replace(\".csv\", \"_hard_only.csv\")\n",
    "hard_df.to_csv(hard_output_path, index=False)\n",
    "print(f\"Saved {len(hard_df)} hard questions to: {hard_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CLEANUP ====================\n",
    "# Free GPU memory\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"GPU memory cleared\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
