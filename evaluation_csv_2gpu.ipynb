{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clarity Model Evaluation - CSV Dataset (2-GPU)\n",
    "Runs evaluation on `clarity_task_evaluation_dataset.csv` using 2 GPUs with FP16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers accelerate datasets scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_recall_fscore_support, \n",
    "    f1_score, \n",
    "    classification_report, \n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*attention_mask.*\")\n",
    "\n",
    "# Check GPUs\n",
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    print(f\"  GPU {i}: {props.name} ({props.total_memory / 1024**3:.1f} GB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "MODEL_NAME = \"Frenzyknight/Clarity-llama-70b\"  # Your model\n",
    "DATA_PATH = \"clarity_task_evaluation_dataset.csv\"  # Input CSV\n",
    "OUTPUT_PATH = \"clarity_csv_predictions.csv\"  # Output CSV\n",
    "\n",
    "MAX_NEW_TOKENS = 600\n",
    "TEMPERATURE = 0.1\n",
    "MAX_INPUT_LENGTH = 3500\n",
    "LIMIT = None  # Set to a number for testing (e.g., 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== PROMPTS ====================\n",
    "SYSTEM_PROMPT = \"You are an expert political discourse analyst. Analyze political interviews step by step and classify response clarity.\"\n",
    "\n",
    "USER_TEMPLATE = \"\"\"Based on a segment of the interview where the interviewer asks a series of questions, classify the type of response provided by the interviewee for the following question.\n",
    "\n",
    "### Classification Categories ###\n",
    "1. Clear Reply - The information requested is explicitly stated (in the requested form)\n",
    "2. Clear Non-Reply - The information requested is not given at all due to ignorance, need for clarification, or declining to answer\n",
    "3. Ambivalent - The information requested is given in an incomplete way (e.g., the answer is too general, partial, implicit, dodging, or deflection)\n",
    "\n",
    "### Full Interview Question ###\n",
    "{interview_question}\n",
    "\n",
    "### Full Interview Answer ###\n",
    "{interview_answer}\n",
    "\n",
    "### Specific Question to Classify ###\n",
    "{question}\n",
    "\n",
    "Think step by step, then provide your classification.\"\"\"\n",
    "\n",
    "\n",
    "def format_messages(row):\n",
    "    \"\"\"Format a CSV row into conversation messages.\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_TEMPLATE.format(\n",
    "            interview_question=str(row[\"interview_question\"]),\n",
    "            interview_answer=str(row[\"interview_answer\"]),\n",
    "            question=str(row[\"question\"])\n",
    "        )}\n",
    "    ]\n",
    "\n",
    "\n",
    "def extract_label(response: str) -> str:\n",
    "    \"\"\"Extract label from model response.\"\"\"\n",
    "    # Look for explicit LABEL: pattern\n",
    "    match = re.search(r'LABEL:\\s*(Clear Reply|Clear Non-Reply|Ambivalent)', response, re.IGNORECASE)\n",
    "    if match:\n",
    "        label = match.group(1).strip().lower()\n",
    "        if \"non-reply\" in label:\n",
    "            return \"Clear Non-Reply\"\n",
    "        elif \"clear reply\" in label:\n",
    "            return \"Clear Reply\"\n",
    "        elif \"ambivalent\" in label:\n",
    "            return \"Ambivalent\"\n",
    "    \n",
    "    # Fallback: check last 100 chars\n",
    "    last_part = response[-100:].lower() if len(response) > 100 else response.lower()\n",
    "    if \"clear non-reply\" in last_part or \"non-reply\" in last_part:\n",
    "        return \"Clear Non-Reply\"\n",
    "    elif \"clear reply\" in last_part:\n",
    "        return \"Clear Reply\"\n",
    "    elif \"ambivalent\" in last_part:\n",
    "        return \"Ambivalent\"\n",
    "    \n",
    "    return \"PARSE_ERROR\"\n",
    "\n",
    "\n",
    "print(\"âœ… Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== LOAD MODEL (2 GPUs) ====================\n",
    "print(\"=\" * 60)\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"Distributing across GPUs with FP16...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Model loaded!\")\n",
    "if hasattr(model, 'hf_device_map'):\n",
    "    devices = set(model.hf_device_map.values())\n",
    "    print(f\"Distributed across: {devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== LOAD CSV DATA ====================\n",
    "print(f\"Loading data from: {DATA_PATH}\")\n",
    "eval_df = pd.read_csv(DATA_PATH)\n",
    "print(f\"âœ… Loaded {len(eval_df)} examples\")\n",
    "print(f\"   Columns: {eval_df.columns.tolist()}\")\n",
    "\n",
    "if LIMIT:\n",
    "    print(f\"\\nâš ï¸  Limiting to first {LIMIT} examples\")\n",
    "    eval_df = eval_df.head(LIMIT)\n",
    "\n",
    "# Preview\n",
    "print(\"\\nSample row:\")\n",
    "print(f\"  Question: {eval_df.iloc[0]['question'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== RUN INFERENCE ====================\n",
    "results = []\n",
    "total = len(eval_df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Running inference on {total} examples\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "for idx, row in tqdm(eval_df.iterrows(), total=total, desc=\"Evaluating\"):\n",
    "    # Format messages from CSV row\n",
    "    messages = format_messages(row)\n",
    "    \n",
    "    # Apply chat template (get string), then tokenize\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = encoded[\"input_ids\"]\n",
    "    input_len = inputs.shape[-1]\n",
    "    \n",
    "    # Skip if too long\n",
    "    if input_len > MAX_INPUT_LENGTH:\n",
    "        tqdm.write(f\"[{idx + 1}/{total}] SKIPPED (too long: {input_len} tokens)\")\n",
    "        results.append({\n",
    "            \"index\": row.get(\"index\", idx),\n",
    "            \"question\": row[\"question\"],\n",
    "            \"prediction\": \"SKIPPED\",\n",
    "            \"raw_output\": f\"Input too long: {input_len} tokens\"\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        inputs = inputs.to(model.device)\n",
    "        attention_mask = torch.ones_like(inputs)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                temperature=TEMPERATURE,\n",
    "                do_sample=TEMPERATURE > 0,\n",
    "                use_cache=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        full_response = tokenizer.decode(\n",
    "            outputs[0, input_len:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        \n",
    "        prediction = extract_label(full_response)\n",
    "        \n",
    "        results.append({\n",
    "            \"index\": row.get(\"index\", idx),\n",
    "            \"question\": row[\"question\"],\n",
    "            \"prediction\": prediction,\n",
    "            \"raw_output\": full_response\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"[{idx + 1}/{total}] ERROR: {e}\")\n",
    "        results.append({\n",
    "            \"index\": row.get(\"index\", idx),\n",
    "            \"question\": row[\"question\"],\n",
    "            \"prediction\": \"ERROR\",\n",
    "            \"raw_output\": str(e)\n",
    "        })\n",
    "    \n",
    "    if (idx + 1) % 25 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nâœ… Inference complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== SAVE RESULTS ====================\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Add ground truth if available\n",
    "if \"clarity_label\" in eval_df.columns:\n",
    "    results_df[\"clarity_label\"] = eval_df[\"clarity_label\"].values[:len(results_df)]\n",
    "\n",
    "results_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"âœ… Saved {len(results_df)} predictions to {OUTPUT_PATH}\")\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== SUMMARY STATISTICS ====================\n",
    "parse_errors = sum(1 for r in results if r['prediction'] == 'PARSE_ERROR')\n",
    "skipped = sum(1 for r in results if r['prediction'] == 'SKIPPED')\n",
    "errors = sum(1 for r in results if r['prediction'] == 'ERROR')\n",
    "successful = len(results) - parse_errors - skipped - errors\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal:        {len(results)}\")\n",
    "print(f\"Successful:   {successful}\")\n",
    "print(f\"Parse errors: {parse_errors}\")\n",
    "print(f\"Skipped:      {skipped}\")\n",
    "print(f\"Errors:       {errors}\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Prediction distribution:\")\n",
    "for label in [\"Clear Reply\", \"Clear Non-Reply\", \"Ambivalent\"]:\n",
    "    count = sum(1 for r in results if r['prediction'] == label)\n",
    "    pct = count / len(results) * 100 if results else 0\n",
    "    print(f\"   {label}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== METRICS (if ground truth available) ====================\n",
    "if \"clarity_label\" in results_df.columns:\n",
    "    # Filter valid labels\n",
    "    y_true = results_df[\"clarity_label\"].astype(str).str.strip()\n",
    "    y_pred = results_df[\"prediction\"].str.strip()\n",
    "    \n",
    "    VALID = {\"Clear Reply\", \"Clear Non-Reply\", \"Ambivalent\"}\n",
    "    mask = y_true.isin(VALID) & y_pred.isin(VALID)\n",
    "    \n",
    "    if mask.sum() > 0:\n",
    "        y_true_f = y_true[mask]\n",
    "        y_pred_f = y_pred[mask]\n",
    "        \n",
    "        print(f\"\\nValid for metrics: {mask.sum()} / {len(mask)}\")\n",
    "        \n",
    "        accuracy = accuracy_score(y_true_f, y_pred_f)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true_f, y_pred_f, average=\"macro\")\n",
    "        f1_w = f1_score(y_true_f, y_pred_f, average=\"weighted\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"METRICS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Accuracy:         {accuracy:.4f}\")\n",
    "        print(f\"Precision (Mac):  {precision:.4f}\")\n",
    "        print(f\"Recall (Mac):     {recall:.4f}\")\n",
    "        print(f\"F1 (Macro):       {f1:.4f}\")\n",
    "        print(f\"F1 (Weighted):    {f1_w:.4f}\")\n",
    "        \n",
    "        print(\"\\n\" + classification_report(y_true_f, y_pred_f, digits=4))\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  No valid predictions for metrics calculation\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No ground truth labels in CSV - skipping metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== SAMPLE OUTPUTS ====================\n",
    "print(\"\\nðŸ” Sample predictions (first 5):\")\n",
    "print(\"-\" * 80)\n",
    "for i in range(min(5, len(results_df))):\n",
    "    row = results_df.iloc[i]\n",
    "    q = str(row['question'])[:60] + \"...\" if len(str(row['question'])) > 60 else row['question']\n",
    "    raw = str(row['raw_output'])[:80] + \"...\" if len(str(row['raw_output'])) > 80 else row['raw_output']\n",
    "    print(f\"{i+1}. Prediction: {row['prediction']}\")\n",
    "    print(f\"   Question: {q}\")\n",
    "    print(f\"   Raw: {raw}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CLEANUP ====================\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"âœ… GPU memory cleared\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
