{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Clarity Classification with GRPO (Reinforcement Learning)\n",
        "\n",
        "This notebook trains a Qwen3-4B model using:\n",
        "1. **Phase 1 (SFT)**: Pre-fine-tune on CoT dataset to learn reasoning format\n",
        "2. **Phase 2 (GRPO)**: Reinforcement learning with classification accuracy rewards\n",
        "\n",
        "Task: Classify political interview responses as:\n",
        "- Clear Reply\n",
        "- Clear Non-Reply  \n",
        "- Ambivalent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FIX: Clean reinstall of typing_extensions to fix corrupted package\n",
        "# =============================================================================\n",
        "print(\"=== Step 1: Completely removing typing_extensions ===\")\n",
        "!pip uninstall typing_extensions -y\n",
        "!pip cache purge\n",
        "# Remove any stale files\n",
        "!rm -rf /usr/local/lib/python3.11/dist-packages/typing_extensions*\n",
        "!rm -rf /root/.cache/pip\n",
        "print()\n",
        "\n",
        "print(\"=== Step 2: Fresh install of typing_extensions ===\")\n",
        "!pip install --no-cache-dir \"typing_extensions>=4.10.0\"\n",
        "print()\n",
        "\n",
        "print(\"=== Step 3: Verify installation ===\")\n",
        "!pip show typing_extensions | grep -E \"Version|Location\"\n",
        "# Check the actual package structure\n",
        "!python3 -c \"import typing_extensions; print('Package file:', typing_extensions.__file__); print('Has TypeIs:', hasattr(typing_extensions, 'TypeIs'))\"\n",
        "print()\n",
        "\n",
        "import os\n",
        "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\"\n",
        "\n",
        "# Detect environment\n",
        "is_colab = \"COLAB_\" in \"\".join(os.environ.keys())\n",
        "print(f\"=== Environment: {'Google Colab' if is_colab else 'Local/Other'} ===\")\n",
        "print()\n",
        "\n",
        "if not is_colab:\n",
        "    print(\"Installing unsloth and vllm...\")\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    print(\"Installing via uv (Colab path)...\")\n",
        "    !pip install --upgrade -qqq uv\n",
        "    !uv pip install vllm==0.11.2 unsloth-zoo unsloth\n",
        "    !uv pip install transformers==4.56.2\n",
        "    !uv pip install --no-deps trl==0.22.2\n",
        "\n",
        "print()\n",
        "print(\"=== Step 4: Re-verify typing_extensions after other installs ===\")\n",
        "!pip show typing_extensions | grep -E \"Version|Location\"\n",
        "!python3 -c \"import typing_extensions; print('Has TypeIs:', hasattr(typing_extensions, 'TypeIs'))\"\n",
        "print()\n",
        "\n",
        "# If still broken, force reinstall again\n",
        "print(\"=== Step 5: Final verification/fix ===\")\n",
        "!pip install --no-cache-dir --force-reinstall \"typing_extensions>=4.10.0\"\n",
        "print()\n",
        "\n",
        "# Final test\n",
        "print(\"=== FINAL TEST: TypeIs import ===\")\n",
        "try:\n",
        "    # Force Python to reload the module\n",
        "    import importlib\n",
        "    import typing_extensions\n",
        "    importlib.reload(typing_extensions)\n",
        "    from typing_extensions import TypeIs\n",
        "    print(\"SUCCESS: TypeIs is importable!\")\n",
        "except ImportError as e:\n",
        "    print(f\"FAILED: {e}\")\n",
        "    print(\"\\n*** IMPORTANT: You MUST restart the runtime now! ***\")\n",
        "    print(\"Go to: Runtime -> Restart runtime, then run this cell again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Model with Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 4096  # Increased for longer CoT reasoning\n",
        "lora_rank = 32\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    fast_inference=False,  # Disabled due to vLLM/LoRA version incompatibility\n",
        "    max_lora_rank=lora_rank,\n",
        "    # gpu_memory_utilization=0.9,  # Only needed with fast_inference=True\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=lora_rank,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=lora_rank * 2,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chat Template for Clarity Classification\n",
        "\n",
        "We use a chat template that encourages step-by-step reasoning before outputting the final classification label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification format markers\n",
        "solution_start = \"LABEL: \"\n",
        "\n",
        "# System prompt for the task\n",
        "system_prompt = \"You are an expert political discourse analyst. Analyze political interviews step by step and classify response clarity.\"\n",
        "\n",
        "# Valid labels for classification\n",
        "VALID_LABELS = [\"Clear Reply\", \"Clear Non-Reply\", \"Ambivalent\"]\n",
        "\n",
        "print(f\"System prompt: {system_prompt}\")\n",
        "print(f\"Valid labels: {VALID_LABELS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chat template for clarity classification\n",
        "# This template handles multi-turn conversations with system/user/assistant roles\n",
        "chat_template = \\\n",
        "    \"{% if messages[0]['role'] == 'system' %}\" \\\n",
        "        \"{{ messages[0]['content'] + eos_token }}\" \\\n",
        "        \"{% set loop_messages = messages[1:] %}\" \\\n",
        "    \"{% else %}\" \\\n",
        "        \"{{ '\" + system_prompt + \"' + eos_token }}\" \\\n",
        "        \"{% set loop_messages = messages %}\" \\\n",
        "    \"{% endif %}\" \\\n",
        "    \"{% for message in loop_messages %}\" \\\n",
        "        \"{% if message['role'] == 'user' %}\" \\\n",
        "            \"{{ message['content'] }}\" \\\n",
        "        \"{% elif message['role'] == 'assistant' %}\" \\\n",
        "            \"{{ message['content'] + eos_token }}\" \\\n",
        "        \"{% endif %}\" \\\n",
        "    \"{% endfor %}\" \\\n",
        "    \"{% if add_generation_prompt %}{{ '' }}{% endif %}\"\n",
        "\n",
        "tokenizer.chat_template = chat_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the chat template\n",
        "test_messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": \"Classify this response...\\n\\nQuestion: What is your policy?\\nAnswer: We are working on it.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Step 1 - The question asks for a specific policy.\\nStep 2 - The answer is vague and does not provide details.\\n\\nLABEL: Ambivalent\"},\n",
        "]\n",
        "\n",
        "print(tokenizer.apply_chat_template(test_messages, tokenize=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 1: Supervised Fine-Tuning (SFT) on CoT Dataset\n",
        "\n",
        "First, we pre-fine-tune the model on the Chain-of-Thought dataset to learn the reasoning format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CoT dataset\n",
        "cot_dataset = load_dataset(\"json\", data_files=\"cot_data/train_cot.jsonl\", split=\"train\")\n",
        "print(f\"Loaded {len(cot_dataset)} CoT examples\")\n",
        "print(f\"Features: {cot_dataset.features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Look at the first example\n",
        "print(\"Example conversation:\")\n",
        "for msg in cot_dataset[0][\"conversations\"]:\n",
        "    print(f\"\\n[{msg['role'].upper()}]:\")\n",
        "    print(msg['content'][:500] + \"...\" if len(msg['content']) > 500 else msg['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format CoT dataset for SFT\n",
        "def format_cot_for_sft(example):\n",
        "    \"\"\"Convert conversations to tokenized text for SFT\"\"\"\n",
        "    conversations = example[\"conversations\"]\n",
        "    text = tokenizer.apply_chat_template(conversations, tokenize=False)\n",
        "    return {\"text\": text}\n",
        "\n",
        "sft_dataset = cot_dataset.map(format_cot_for_sft)\n",
        "print(f\"SFT dataset size: {len(sft_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check token lengths and filter out very long examples\n",
        "def get_token_length(example):\n",
        "    return {\"token_length\": len(tokenizer.encode(example[\"text\"]))}\n",
        "\n",
        "sft_dataset = sft_dataset.map(get_token_length)\n",
        "\n",
        "import numpy as np\n",
        "lengths = np.array(sft_dataset[\"token_length\"])\n",
        "print(f\"Token length stats: min={lengths.min()}, max={lengths.max()}, mean={lengths.mean():.0f}, median={np.median(lengths):.0f}\")\n",
        "\n",
        "# Filter to keep examples that fit in context\n",
        "max_sft_length = min(max_seq_length, int(np.percentile(lengths, 95)))\n",
        "print(f\"Using max SFT length: {max_sft_length}\")\n",
        "\n",
        "sft_dataset = sft_dataset.filter(lambda x: x[\"token_length\"] <= max_sft_length)\n",
        "print(f\"Filtered SFT dataset size: {len(sft_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "sft_trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=sft_dataset,\n",
        "    args=SFTConfig(\n",
        "        dataset_text_field=\"text\",\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=10,\n",
        "        num_train_epochs=1,  # 1 epoch for format learning\n",
        "        learning_rate=2e-5,\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.001,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs_sft\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run SFT training\n",
        "print(\"Starting SFT training...\")\n",
        "sft_trainer.train()\n",
        "print(\"SFT training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test if model learned the format\n",
        "test_prompt = cot_dataset[0][\"conversations\"][:2]  # System + user only\n",
        "text = tokenizer.apply_chat_template(test_prompt, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "from transformers import TextStreamer\n",
        "print(\"Testing model output after SFT:\")\n",
        "print(\"=\" * 50)\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
        "    temperature=0.7,\n",
        "    max_new_tokens=1024,\n",
        "    streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup before GRPO\n",
        "del sft_dataset, sft_trainer\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 2: GRPO (Reinforcement Learning)\n",
        "\n",
        "Now we use GRPO with classification accuracy as the reward signal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the training data with ground truth labels for GRPO\n",
        "grpo_raw_dataset = load_dataset(\"parquet\", data_files=\"data/train/train.parquet\", split=\"train\")\n",
        "print(f\"Loaded {len(grpo_raw_dataset)} examples for GRPO\")\n",
        "print(f\"Features: {grpo_raw_dataset.features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Look at the data structure\n",
        "print(\"Example conversation structure:\")\n",
        "example = grpo_raw_dataset[0]\n",
        "for msg in example[\"conversations\"]:\n",
        "    print(f\"  [{msg['role']}]: {msg['content'][:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_grpo_dataset(example):\n",
        "    \"\"\"Convert dataset to GRPO format with prompts and ground truth answers\"\"\"\n",
        "    conversations = example[\"conversations\"]\n",
        "    \n",
        "    # Extract ground truth label (assistant's response)\n",
        "    answer = None\n",
        "    for msg in conversations:\n",
        "        if msg[\"role\"] == \"assistant\":\n",
        "            answer = msg[\"content\"].strip()\n",
        "            break\n",
        "    \n",
        "    # Extract prompt (system + user messages only)\n",
        "    prompt = [msg for msg in conversations if msg[\"role\"] != \"assistant\"]\n",
        "    \n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"answer\": answer\n",
        "    }\n",
        "\n",
        "grpo_dataset = grpo_raw_dataset.map(prepare_grpo_dataset)\n",
        "print(f\"GRPO dataset prepared: {len(grpo_dataset)} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check label distribution\n",
        "from collections import Counter\n",
        "label_counts = Counter(grpo_dataset[\"answer\"])\n",
        "print(\"Label distribution:\")\n",
        "for label, count in label_counts.most_common():\n",
        "    print(f\"  {label}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter to valid labels only\n",
        "grpo_dataset = grpo_dataset.filter(lambda x: x[\"answer\"] in VALID_LABELS)\n",
        "print(f\"Filtered GRPO dataset: {len(grpo_dataset)} examples with valid labels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reward Functions for Classification Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Regex to extract the label from model output\n",
        "label_pattern = re.compile(\n",
        "    r\"LABEL:\\s*(Clear Reply|Clear Non-Reply|Ambivalent)\",\n",
        "    flags=re.IGNORECASE\n",
        ")\n",
        "\n",
        "def extract_label(text):\n",
        "    \"\"\"Extract the classification label from model output\"\"\"\n",
        "    match = label_pattern.search(text)\n",
        "    if match:\n",
        "        # Normalize the label\n",
        "        label = match.group(1).strip()\n",
        "        label_map = {\n",
        "            \"clear reply\": \"Clear Reply\",\n",
        "            \"clear non-reply\": \"Clear Non-Reply\",\n",
        "            \"ambivalent\": \"Ambivalent\"\n",
        "        }\n",
        "        return label_map.get(label.lower(), label)\n",
        "    return None\n",
        "\n",
        "# Test the extraction\n",
        "test_outputs = [\n",
        "    \"Step 1... Step 2... LABEL: Clear Reply\",\n",
        "    \"Some reasoning here.\\n\\nLABEL: Ambivalent\",\n",
        "    \"The answer is Clear Non-Reply based on...\",  # No LABEL prefix\n",
        "    \"LABEL: clear reply\",  # lowercase\n",
        "]\n",
        "\n",
        "for out in test_outputs:\n",
        "    print(f\"Input: '{out[:50]}...' -> Extracted: {extract_label(out)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global counter for printing\n",
        "global PRINTED_TIMES\n",
        "PRINTED_TIMES = 0\n",
        "global PRINT_EVERY_STEPS\n",
        "PRINT_EVERY_STEPS = 10\n",
        "\n",
        "def classification_accuracy_reward(prompts, completions, answer, **kwargs):\n",
        "    \"\"\"\n",
        "    Reward function based on classification accuracy.\n",
        "    \n",
        "    Rewards:\n",
        "    - +5.0: Exact match (correct label)\n",
        "    - +4.0: Case-insensitive match\n",
        "    - -1.0: Valid label but wrong\n",
        "    - -3.0: Invalid label format\n",
        "    - -4.0: No label found in output\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    \n",
        "    global PRINTED_TIMES\n",
        "    global PRINT_EVERY_STEPS\n",
        "    \n",
        "    for i, (completion, true_label) in enumerate(zip(completions, answer)):\n",
        "        response = completion[0][\"content\"]\n",
        "        predicted = extract_label(response)\n",
        "        \n",
        "        # Print debug info periodically\n",
        "        if i == 0 and PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n",
        "            print(\"=\" * 50)\n",
        "            print(f\"True label: {true_label}\")\n",
        "            print(f\"Predicted: {predicted}\")\n",
        "            print(f\"Response (last 300 chars): ...{response[-300:]}\")\n",
        "            print(\"=\" * 50)\n",
        "        \n",
        "        if i == 0:\n",
        "            PRINTED_TIMES += 1\n",
        "        \n",
        "        # Compute reward\n",
        "        if predicted is None:\n",
        "            scores.append(-4.0)  # No label found\n",
        "        elif predicted == true_label:\n",
        "            scores.append(5.0)  # Exact match\n",
        "        elif predicted.lower() == true_label.lower():\n",
        "            scores.append(4.0)  # Case-insensitive match\n",
        "        elif predicted in VALID_LABELS:\n",
        "            scores.append(-1.0)  # Valid label but wrong\n",
        "        else:\n",
        "            scores.append(-3.0)  # Invalid label\n",
        "    \n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure and Run GRPO Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate max prompt length based on dataset\n",
        "def get_prompt_length(example):\n",
        "    text = tokenizer.apply_chat_template(example[\"prompt\"], tokenize=False, add_generation_prompt=True)\n",
        "    return {\"prompt_length\": len(tokenizer.encode(text))}\n",
        "\n",
        "grpo_dataset = grpo_dataset.map(get_prompt_length)\n",
        "\n",
        "prompt_lengths = np.array(grpo_dataset[\"prompt_length\"])\n",
        "print(f\"Prompt length stats: min={prompt_lengths.min()}, max={prompt_lengths.max()}, mean={prompt_lengths.mean():.0f}\")\n",
        "\n",
        "max_prompt_length = int(np.percentile(prompt_lengths, 90)) + 10\n",
        "max_completion_length = max_seq_length - max_prompt_length\n",
        "\n",
        "print(f\"Max prompt length: {max_prompt_length}\")\n",
        "print(f\"Max completion length: {max_completion_length}\")\n",
        "\n",
        "# Filter out prompts that are too long\n",
        "grpo_dataset = grpo_dataset.filter(lambda x: x[\"prompt_length\"] <= max_prompt_length)\n",
        "print(f\"Filtered GRPO dataset: {len(grpo_dataset)} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "# GRPO training configuration (without vLLM - using standard HF generation)\n",
        "training_args = GRPOConfig(\n",
        "    # Sampling parameters\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        "    # Training parameters\n",
        "    learning_rate=5e-6,\n",
        "    weight_decay=0.001,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    logging_steps=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_generations=4,  # Number of completions per prompt\n",
        "    max_prompt_length=max_prompt_length,\n",
        "    max_completion_length=max_completion_length,\n",
        "    max_steps=500,  # Adjust based on your needs\n",
        "    save_steps=100,\n",
        "    report_to=\"none\",\n",
        "    output_dir=\"outputs_grpo\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create GRPO trainer\n",
        "grpo_trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=grpo_dataset,\n",
        "    reward_funcs=[classification_accuracy_reward],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run GRPO training\n",
        "print(\"Starting GRPO training...\")\n",
        "print(\"Watch the 'reward' column - it should increase over time!\")\n",
        "print(\"\")\n",
        "grpo_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Inference: Test the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on a few examples\n",
        "test_examples = grpo_dataset.select(range(min(3, len(grpo_dataset))))\n",
        "\n",
        "for i, example in enumerate(test_examples):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Example {i+1}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Get the user message content for context\n",
        "    user_content = [m[\"content\"] for m in example[\"prompt\"] if m[\"role\"] == \"user\"][0]\n",
        "    print(f\"Question excerpt: {user_content[:200]}...\")\n",
        "    print(f\"\\nTrue label: {example['answer']}\")\n",
        "    \n",
        "    # Generate\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        example[\"prompt\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        temperature=0.7,\n",
        "        max_new_tokens=1024,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    predicted = extract_label(response)\n",
        "    \n",
        "    print(f\"\\nModel output (last 500 chars):\")\n",
        "    print(f\"...{response[-500:]}\")\n",
        "    print(f\"\\nExtracted label: {predicted}\")\n",
        "    print(f\"Correct: {predicted == example['answer']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save LoRA adapters\n",
        "model.save_pretrained(\"clarity_grpo_lora\")\n",
        "tokenizer.save_pretrained(\"clarity_grpo_lora\")\n",
        "print(\"Saved LoRA adapters to clarity_grpo_lora/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Merge and save as full model\n",
        "# model.save_pretrained_merged(\"clarity_grpo_merged\", tokenizer, save_method=\"merged_16bit\")\n",
        "# print(\"Saved merged model to clarity_grpo_merged/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Push to Hugging Face Hub\n",
        "# model.push_to_hub_merged(\"your-username/clarity-grpo-qwen3-4b\", tokenizer, save_method=\"merged_16bit\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
