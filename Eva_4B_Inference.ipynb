{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Eva-4B Inference Notebook\n",
        "Inference with FutureMa/Eva-4B (Qwen3 4B Instruct) on the QEvasion test dataset.\n",
        "\n",
        "**Classification Labels:**\n",
        "- `direct`: answers the core question with specific information\n",
        "- `intermediate`: provides related information but sidesteps the core question\n",
        "- `fully_evasive`: does not address the question (refusal, redirection, non-response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install transformers accelerate datasets scikit-learn pandas tqdm -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import warnings\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "# Suppress attention mask warning\n",
        "warnings.filterwarnings(\"ignore\", message=\".*attention_mask.*\")\n",
        "\n",
        "# Check available GPUs\n",
        "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
        "for i in range(torch.cuda.device_count()):\n",
        "    props = torch.cuda.get_device_properties(i)\n",
        "    print(f\"  GPU {i}: {props.name} ({props.total_memory / 1024**3:.1f} GB)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === CONFIGURATION ===\n",
        "MODEL_NAME = \"FutureMa/Eva-4B\"  # Qwen3 4B Instruct model\n",
        "HF_DATASET = \"ailsntua/QEvasion\"  # Test dataset\n",
        "OUTPUT_PATH = \"eva_4b_predictions.csv\"\n",
        "\n",
        "# Generation parameters\n",
        "MAX_NEW_TOKENS = 512\n",
        "TEMPERATURE = 0.1\n",
        "MAX_INPUT_LENGTH = 3500\n",
        "\n",
        "# Set to a number for testing, None for full run\n",
        "LIMIT = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompt Template & Label Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System prompt for classification\n",
        "SYSTEM_PROMPT = \"You are an expert political discourse analyst. Classify response clarity.\"\n",
        "\n",
        "# User prompt template with new classification labels\n",
        "USER_TEMPLATE = \"\"\"Classify the response to the question as one of:\n",
        "1. direct - answers the core question with specific information\n",
        "2. intermediate - provides related information but sidesteps the core question\n",
        "3. fully_evasive - does not address the question (refusal, redirection, non-response)\n",
        "\n",
        "### Interview Question ###\n",
        "{interview_question}\n",
        "\n",
        "### Interview Answer ###\n",
        "{interview_answer}\n",
        "\n",
        "### Specific Question ###\n",
        "{question}\n",
        "\n",
        "Classification:\"\"\"\n",
        "\n",
        "\n",
        "def extract_label(response: str) -> str:\n",
        "    \"\"\"Extract classification label from model response.\"\"\"\n",
        "    response_lower = response.lower().strip()\n",
        "    \n",
        "    # Check for explicit label mentions (prioritize more specific matches)\n",
        "    if \"fully_evasive\" in response_lower or \"fully evasive\" in response_lower:\n",
        "        return \"fully_evasive\"\n",
        "    elif \"intermediate\" in response_lower:\n",
        "        return \"intermediate\"\n",
        "    elif \"direct\" in response_lower:\n",
        "        return \"direct\"\n",
        "    \n",
        "    # Fallback: check first word/line for label\n",
        "    first_word = response_lower.split()[0] if response_lower.split() else \"\"\n",
        "    if first_word in [\"direct\", \"intermediate\", \"fully_evasive\"]:\n",
        "        return first_word\n",
        "    \n",
        "    return \"PARSE_ERROR\"\n",
        "\n",
        "\n",
        "def format_messages(row) -> list:\n",
        "    \"\"\"Format a dataset row into conversation messages.\"\"\"\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": USER_TEMPLATE.format(\n",
        "            interview_question=str(row[\"interview_question\"]),\n",
        "            interview_answer=str(row[\"interview_answer\"]),\n",
        "            question=str(row[\"question\"])\n",
        "        )}\n",
        "    ]\n",
        "\n",
        "\n",
        "# Map ground truth labels to new classification scheme\n",
        "LABEL_MAPPING = {\n",
        "    \"Clear Reply\": \"direct\",\n",
        "    \"Ambivalent\": \"intermediate\",\n",
        "    \"Clear Non-Reply\": \"fully_evasive\",\n",
        "}\n",
        "\n",
        "print(\"Prompt template and helper functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "print(\"Loading in full precision (float16)...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"\\nLoading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model in full precision (float16 for GPU efficiency)\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "print(\"\\nModel loaded successfully!\")\n",
        "print(f\"Model device map: {model.hf_device_map if hasattr(model, 'hf_device_map') else 'auto'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test dataset from HuggingFace\n",
        "print(f\"Loading test dataset from: {HF_DATASET}\")\n",
        "test_dataset = load_dataset(HF_DATASET, split=\"test\")\n",
        "\n",
        "print(f\"Loaded {len(test_dataset)} test examples\")\n",
        "print(f\"Columns: {test_dataset.column_names}\")\n",
        "\n",
        "# Convert to DataFrame for easier handling\n",
        "test_df = test_dataset.to_pandas()\n",
        "\n",
        "if LIMIT:\n",
        "    print(f\"Limiting to first {LIMIT} examples\")\n",
        "    test_df = test_df.head(LIMIT)\n",
        "\n",
        "# Preview sample\n",
        "print(\"\\nSample row:\")\n",
        "print(test_df.iloc[0][[\"question\", \"clarity_label\"]].to_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "total = len(test_df)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"Starting inference on {total} examples\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "for idx, row in tqdm(test_df.iterrows(), total=total, desc=\"Evaluating\"):\n",
        "    messages = format_messages(row)\n",
        "    \n",
        "    # Apply chat template\n",
        "    chat_output = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    \n",
        "    # Handle both tensor and BatchEncoding returns\n",
        "    if isinstance(chat_output, torch.Tensor):\n",
        "        inputs = chat_output\n",
        "    else:\n",
        "        inputs = chat_output[\"input_ids\"]\n",
        "    \n",
        "    input_len = inputs.shape[-1]\n",
        "    \n",
        "    # Skip if input is too long\n",
        "    if input_len > MAX_INPUT_LENGTH:\n",
        "        tqdm.write(f\"[{idx + 1}/{total}] SKIPPED (input too long: {input_len} tokens)\")\n",
        "        results.append({\n",
        "            \"index\": idx,\n",
        "            \"question\": row[\"question\"],\n",
        "            \"prediction\": \"SKIPPED\",\n",
        "            \"ground_truth\": LABEL_MAPPING.get(row.get(\"clarity_label\", \"\"), \"\"),\n",
        "            \"raw_output\": f\"Input too long: {input_len} tokens\"\n",
        "        })\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        inputs = inputs.to(model.device)\n",
        "        attention_mask = torch.ones_like(inputs)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=inputs,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=MAX_NEW_TOKENS,\n",
        "                temperature=TEMPERATURE,\n",
        "                do_sample=TEMPERATURE > 0,\n",
        "                use_cache=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "        \n",
        "        # Decode only the generated part\n",
        "        full_response = tokenizer.decode(\n",
        "            outputs[0, input_len:],\n",
        "            skip_special_tokens=True\n",
        "        ).strip()\n",
        "        \n",
        "        # Extract prediction label\n",
        "        prediction = extract_label(full_response)\n",
        "        \n",
        "        # Map ground truth to new labels\n",
        "        ground_truth = LABEL_MAPPING.get(row.get(\"clarity_label\", \"\"), \"\")\n",
        "        \n",
        "        results.append({\n",
        "            \"index\": idx,\n",
        "            \"question\": row[\"question\"],\n",
        "            \"prediction\": prediction,\n",
        "            \"ground_truth\": ground_truth,\n",
        "            \"raw_output\": full_response,\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        tqdm.write(f\"[{idx + 1}/{total}] ERROR: {e}\")\n",
        "        results.append({\n",
        "            \"index\": idx,\n",
        "            \"question\": row[\"question\"],\n",
        "            \"prediction\": \"ERROR\",\n",
        "            \"ground_truth\": LABEL_MAPPING.get(row.get(\"clarity_label\", \"\"), \"\"),\n",
        "            \"raw_output\": str(e),\n",
        "        })\n",
        "    \n",
        "    # Clear cache periodically\n",
        "    if (idx + 1) % 25 == 0:\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nInference complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to CSV\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(OUTPUT_PATH, index=False)\n",
        "\n",
        "# Summary statistics\n",
        "parse_errors = sum(1 for r in results if r['prediction'] == 'PARSE_ERROR')\n",
        "skipped = sum(1 for r in results if r['prediction'] == 'SKIPPED')\n",
        "errors = sum(1 for r in results if r['prediction'] == 'ERROR')\n",
        "successful = len(results) - parse_errors - skipped - errors\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"INFERENCE COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nSaved {len(results)} predictions to {OUTPUT_PATH}\")\n",
        "\n",
        "print(\"\\nSummary:\")\n",
        "print(f\"  Total:         {len(results)}\")\n",
        "print(f\"  Successful:    {successful}\")\n",
        "print(f\"  Parse errors:  {parse_errors}\")\n",
        "print(f\"  Skipped:       {skipped}\")\n",
        "print(f\"  Errors:        {errors}\")\n",
        "\n",
        "print(\"\\nPrediction distribution:\")\n",
        "for label in [\"direct\", \"intermediate\", \"fully_evasive\"]:\n",
        "    count = sum(1 for r in results if r['prediction'] == label)\n",
        "    pct = count / len(results) * 100 if results else 0\n",
        "    print(f\"  {label}: {count} ({pct:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter to valid predictions only\n",
        "VALID_LABELS = {\"direct\", \"intermediate\", \"fully_evasive\"}\n",
        "\n",
        "y_pred = results_df[\"prediction\"]\n",
        "y_true = results_df[\"ground_truth\"]\n",
        "\n",
        "# Create mask for valid predictions\n",
        "mask = y_pred.isin(VALID_LABELS) & y_true.isin(VALID_LABELS)\n",
        "\n",
        "y_pred_filtered = y_pred[mask]\n",
        "y_true_filtered = y_true[mask]\n",
        "\n",
        "print(f\"Valid predictions: {mask.sum()} / {len(mask)}\")\n",
        "print(f\"Filtered out: {(~mask).sum()} (PARSE_ERROR, SKIPPED, ERROR, or missing ground truth)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate metrics\n",
        "if len(y_pred_filtered) > 0:\n",
        "    # Accuracy\n",
        "    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
        "    \n",
        "    # Precision / Recall / F1 (Macro)\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        y_true_filtered,\n",
        "        y_pred_filtered,\n",
        "        average=\"macro\",\n",
        "        zero_division=0\n",
        "    )\n",
        "    \n",
        "    # Weighted F1\n",
        "    f1_weighted = f1_score(\n",
        "        y_true_filtered,\n",
        "        y_pred_filtered,\n",
        "        average=\"weighted\",\n",
        "        zero_division=0\n",
        "    )\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"EVALUATION METRICS (Eva-4B on QEvasion Test Set)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nAccuracy        : {accuracy:.4f}\")\n",
        "    print(f\"Precision (Mac) : {precision_macro:.4f}\")\n",
        "    print(f\"Recall (Mac)    : {recall_macro:.4f}\")\n",
        "    print(f\"F1 (Macro)      : {f1_macro:.4f}\")\n",
        "    print(f\"F1 (Weighted)   : {f1_weighted:.4f}\")\n",
        "else:\n",
        "    print(\"No valid predictions to evaluate!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed classification report\n",
        "if len(y_pred_filtered) > 0:\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(classification_report(\n",
        "        y_true_filtered,\n",
        "        y_pred_filtered,\n",
        "        labels=list(VALID_LABELS),\n",
        "        zero_division=0\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "if len(y_pred_filtered) > 0:\n",
        "    labels = [\"direct\", \"intermediate\", \"fully_evasive\"]\n",
        "    cm = confusion_matrix(y_true_filtered, y_pred_filtered, labels=labels)\n",
        "    \n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'':20} {'Predicted':^45}\")\n",
        "    print(f\"{'Actual':20} {labels[0]:^15} {labels[1]:^15} {labels[2]:^15}\")\n",
        "    print(\"-\" * 60)\n",
        "    for i, label in enumerate(labels):\n",
        "        print(f\"{label:20} {cm[i, 0]:^15} {cm[i, 1]:^15} {cm[i, 2]:^15}\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show sample outputs\n",
        "print(\"Sample predictions (first 5):\")\n",
        "print(\"-\" * 60)\n",
        "for i, r in enumerate(results[:5]):\n",
        "    raw = r.get('raw_output', 'N/A')[:80]\n",
        "    match = \"CORRECT\" if r['prediction'] == r['ground_truth'] else \"WRONG\"\n",
        "    print(f\"{i+1}. Pred: {r['prediction']:15} | GT: {r['ground_truth']:15} | {match}\")\n",
        "    print(f\"   Raw: '{raw}...'\")\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
