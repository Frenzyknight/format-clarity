{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clarity Model Evaluation (COT)\n",
        "This notebook evaluates the Clarity model with Chain-of-Thought prompting on:\n",
        "1. Custom evaluation CSV dataset\n",
        "2. HuggingFace test dataset with metrics\n",
        "\n",
        "Uses 8-bit quantization (bitsandbytes) for single GPU inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install transformers accelerate datasets scikit-learn bitsandbytes -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "import warnings\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "# Suppress attention mask warning\n",
        "warnings.filterwarnings(\"ignore\", message=\".*attention_mask.*\")\n",
        "\n",
        "# Check available GPUs\n",
        "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
        "for i in range(torch.cuda.device_count()):\n",
        "    props = torch.cuda.get_device_properties(i)\n",
        "    print(f\"  GPU {i}: {props.name} ({props.total_memory / 1024**3:.1f} GB)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === CONFIGURATION ===\n",
        "MODEL_NAME = \"Frenzyknight/Clarity-llama-70b\"  # Your fine-tuned model\n",
        "DATA_PATH = \"workspace/clarity_task_evaluation_dataset.csv\"  # Custom eval data\n",
        "OUTPUT_PATH = \"clarity_eval_predictions_cot.csv\"  # Output for custom eval\n",
        "HF_OUTPUT_PATH = \"hf_test_predictions_cot.csv\"  # Output for HF test\n",
        "MAX_NEW_TOKENS = 1024  # COT model outputs reasoning + label\n",
        "TEMPERATURE = 0.1\n",
        "MAX_INPUT_LENGTH = 3500\n",
        "LIMIT = None  # Set to 5 for testing, None for full run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System prompt (COT)\n",
        "SYSTEM_PROMPT = \"You are an expert political discourse analyst. Analyze political interviews step by step and classify response clarity.\"\n",
        "\n",
        "# User prompt template (COT - with \"Think step by step\")\n",
        "USER_TEMPLATE = \"\"\"Based on a segment of the interview where the interviewer asks a series of questions, classify the type of response provided by the interviewee for the following question.\n",
        "\n",
        "### Classification Categories ###\n",
        "1. Clear Reply - The information requested is explicitly stated (in the requested form)\n",
        "2. Clear Non-Reply - The information requested is not given at all due to ignorance, need for clarification, or declining to answer\n",
        "3. Ambivalent - The information requested is given in an incomplete way (e.g., the answer is too general, partial, implicit, dodging, or deflection)\n",
        "\n",
        "### Full Interview Question ###\n",
        "{interview_question}\n",
        "\n",
        "### Full Interview Answer ###\n",
        "{interview_answer}\n",
        "\n",
        "### Specific Question to Classify ###\n",
        "{question}\n",
        "\n",
        "Think step by step, then provide your classification.\"\"\"\n",
        "\n",
        "\n",
        "def extract_label(response: str) -> str:\n",
        "    \"\"\"Extract label from COT response (looks for 'LABEL: <classification>').\"\"\"\n",
        "    \n",
        "    # Look for explicit LABEL: pattern (case insensitive)\n",
        "    label_match = re.search(r'LABEL:\\s*(Clear Reply|Clear Non-Reply|Ambivalent)', response, re.IGNORECASE)\n",
        "    if label_match:\n",
        "        label = label_match.group(1).strip()\n",
        "        # Normalize capitalization\n",
        "        if \"non-reply\" in label.lower():\n",
        "            return \"Clear Non-Reply\"\n",
        "        elif \"clear reply\" in label.lower():\n",
        "            return \"Clear Reply\"\n",
        "        elif \"ambivalent\" in label.lower():\n",
        "            return \"Ambivalent\"\n",
        "    \n",
        "    # Fallback: check last 100 chars for label mentions\n",
        "    last_part = response[-100:].lower() if len(response) > 100 else response.lower()\n",
        "    if \"clear non-reply\" in last_part or \"non-reply\" in last_part:\n",
        "        return \"Clear Non-Reply\"\n",
        "    elif \"clear reply\" in last_part:\n",
        "        return \"Clear Reply\"\n",
        "    elif \"ambivalent\" in last_part:\n",
        "        return \"Ambivalent\"\n",
        "    \n",
        "    return \"PARSE_ERROR\"\n",
        "\n",
        "\n",
        "def format_messages_csv(row):\n",
        "    \"\"\"Format a CSV row into conversation messages.\"\"\"\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": USER_TEMPLATE.format(\n",
        "            interview_question=str(row[\"interview_question\"]),\n",
        "            interview_answer=str(row[\"interview_answer\"]),\n",
        "            question=str(row[\"question\"])\n",
        "        )}\n",
        "    ]\n",
        "\n",
        "print(\"âœ… Helper functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "print(\"Using 4-bit quantization for single GPU...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 4-bit quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"\\nLoading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model with 4-bit quantization\n",
        "print(\"Loading model with 4-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Model loaded successfully (4-bit quantized)!\")\n",
        "print(f\"Model device map: {model.hf_device_map if hasattr(model, 'hf_device_map') else 'auto'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part A: Custom CSV Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"ðŸ“‚ Loading data from: {DATA_PATH}\")\n",
        "eval_df = pd.read_csv(DATA_PATH)\n",
        "print(f\"   Loaded {len(eval_df)} examples\")\n",
        "print(f\"   Columns: {eval_df.columns.tolist()}\")\n",
        "\n",
        "if LIMIT:\n",
        "    print(f\"   Limiting to first {LIMIT} examples\")\n",
        "    eval_df = eval_df.head(LIMIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "csv_results = []\n",
        "total = len(eval_df)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"Starting inference on {total} examples (Custom CSV)\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "for idx, row in tqdm(eval_df.iterrows(), total=total, desc=\"Evaluating CSV\"):\n",
        "    messages = format_messages_csv(row)\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    \n",
        "    input_len = inputs.shape[-1]\n",
        "    \n",
        "    if input_len > MAX_INPUT_LENGTH:\n",
        "        tqdm.write(f\"[{idx + 1}/{total}] SKIPPED (input too long: {input_len} tokens)\")\n",
        "        csv_results.append({\n",
        "            \"index\": row.get(\"index\", idx),\n",
        "            \"question\": row[\"question\"],\n",
        "            \"prediction\": \"SKIPPED\",\n",
        "            \"raw_output\": f\"Input too long: {input_len} tokens\"\n",
        "        })\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        inputs = inputs.to(model.device)\n",
        "        attention_mask = torch.ones_like(inputs)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=inputs,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=MAX_NEW_TOKENS,\n",
        "                temperature=TEMPERATURE,\n",
        "                do_sample=TEMPERATURE > 0,\n",
        "                use_cache=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "        \n",
        "        full_response = tokenizer.decode(\n",
        "            outputs[0, input_len:],\n",
        "            skip_special_tokens=True\n",
        "        ).strip()\n",
        "        \n",
        "        prediction = extract_label(full_response)\n",
        "        \n",
        "        csv_results.append({\n",
        "            \"index\": row.get(\"index\", idx),\n",
        "            \"question\": row[\"question\"],\n",
        "            \"prediction\": prediction,\n",
        "            \"raw_output\": full_response,\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        tqdm.write(f\"[{idx + 1}/{total}] ERROR: {e}\")\n",
        "        csv_results.append({\n",
        "            \"index\": row.get(\"index\", idx),\n",
        "            \"question\": row[\"question\"],\n",
        "            \"prediction\": \"ERROR\",\n",
        "            \"raw_output\": str(e),\n",
        "        })\n",
        "    \n",
        "    if (idx + 1) % 25 == 0:\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nâœ… CSV Inference complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "csv_results_df = pd.DataFrame(csv_results)\n",
        "csv_results_df.to_csv(OUTPUT_PATH, index=False)\n",
        "\n",
        "parse_errors = sum(1 for r in csv_results if r['prediction'] == 'PARSE_ERROR')\n",
        "skipped = sum(1 for r in csv_results if r['prediction'] == 'SKIPPED')\n",
        "errors = sum(1 for r in csv_results if r['prediction'] == 'ERROR')\n",
        "successful = len(csv_results) - parse_errors - skipped - errors\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CSV EVALUATION COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nâœ… Saved {len(csv_results)} predictions to {OUTPUT_PATH}\")\n",
        "\n",
        "print(\"\\nðŸ“Š Summary:\")\n",
        "print(f\"   Total:         {len(csv_results)}\")\n",
        "print(f\"   Successful:    {successful}\")\n",
        "print(f\"   Parse errors:  {parse_errors}\")\n",
        "print(f\"   Skipped:       {skipped}\")\n",
        "print(f\"   Errors:        {errors}\")\n",
        "\n",
        "print(\"\\nðŸ“ˆ Prediction distribution:\")\n",
        "for label in [\"Clear Reply\", \"Clear Non-Reply\", \"Ambivalent\"]:\n",
        "    count = sum(1 for r in csv_results if r['prediction'] == label)\n",
        "    pct = count / len(csv_results) * 100 if csv_results else 0\n",
        "    print(f\"   {label}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "print(\"\\nðŸ” Sample outputs (first 5):\")\n",
        "for i, r in enumerate(csv_results[:5]):\n",
        "    raw = r.get('raw_output', 'N/A')[:40]\n",
        "    print(f\"   {i+1}. {r['prediction']} | Raw: '{raw}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part B: HuggingFace Test Dataset (with Metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test dataset from HuggingFace\n",
        "hf_test = load_dataset(\"Frenzyknight/clarity-dataset\", split=\"test\")\n",
        "print(f\"âœ… Loaded {len(hf_test)} test examples from HuggingFace\")\n",
        "print(f\"   Columns: {hf_test.column_names}\")\n",
        "print(f\"   Sample conversation structure: {len(hf_test[0]['conversations'])} messages\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hf_results = []\n",
        "total = len(hf_test)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"Running inference on HuggingFace test set ({total} examples)\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "for idx, example in tqdm(enumerate(hf_test), total=total, desc=\"Evaluating HF\"):\n",
        "    # Get the conversation (system + user messages already formatted)\n",
        "    messages = example[\"conversations\"]\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    \n",
        "    input_len = inputs.shape[-1]\n",
        "    \n",
        "    if input_len > MAX_INPUT_LENGTH:\n",
        "        tqdm.write(f\"[{idx + 1}/{total}] SKIPPED (input too long: {input_len} tokens)\")\n",
        "        hf_results.append({\"prediction\": \"SKIPPED\", \"raw_output\": \"Too long\"})\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        inputs = inputs.to(model.device)\n",
        "        attention_mask = torch.ones_like(inputs)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=inputs,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=MAX_NEW_TOKENS,\n",
        "                temperature=TEMPERATURE,\n",
        "                do_sample=TEMPERATURE > 0,\n",
        "                use_cache=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "        \n",
        "        full_response = tokenizer.decode(\n",
        "            outputs[0, input_len:],\n",
        "            skip_special_tokens=True\n",
        "        ).strip()\n",
        "        \n",
        "        prediction = extract_label(full_response)\n",
        "        hf_results.append({\"prediction\": prediction, \"raw_output\": full_response})\n",
        "        \n",
        "    except Exception as e:\n",
        "        tqdm.write(f\"[{idx + 1}/{total}] ERROR: {e}\")\n",
        "        hf_results.append({\"prediction\": \"ERROR\", \"raw_output\": str(e)})\n",
        "    \n",
        "    if (idx + 1) % 25 == 0:\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nâœ… HuggingFace Inference complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load ground truth from QEvasion dataset (has clarity_label)\n",
        "qevasion_test = load_dataset(\"ailsntua/QEvasion\", split=\"test\")\n",
        "\n",
        "# Create DataFrame with predictions and ground truth\n",
        "hf_df = pd.DataFrame(hf_results)\n",
        "hf_df[\"clarity_label\"] = [row[\"clarity_label\"] for row in qevasion_test]\n",
        "hf_df[\"annotator1\"] = [row[\"annotator1\"] for row in qevasion_test]\n",
        "hf_df[\"annotator2\"] = [row[\"annotator2\"] for row in qevasion_test]\n",
        "hf_df[\"annotator3\"] = [row[\"annotator3\"] for row in qevasion_test]\n",
        "\n",
        "# Save to CSV\n",
        "hf_df.to_csv(HF_OUTPUT_PATH, index=False)\n",
        "print(f\"âœ… Saved predictions with labels to {HF_OUTPUT_PATH}\")\n",
        "print(f\"\\nDataFrame shape: {hf_df.shape}\")\n",
        "hf_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter to valid labels only\n",
        "y_true = hf_df[\"clarity_label\"].str.strip()\n",
        "y_pred = hf_df[\"prediction\"].str.strip()\n",
        "\n",
        "VALID = {\"Clear Reply\", \"Clear Non-Reply\", \"Ambivalent\"}\n",
        "mask = y_true.isin(VALID) & y_pred.isin(VALID)\n",
        "\n",
        "y_true_filtered = y_true[mask]\n",
        "y_pred_filtered = y_pred[mask]\n",
        "\n",
        "print(f\"Valid predictions: {mask.sum()} / {len(mask)}\")\n",
        "print(f\"Filtered out: {(~mask).sum()} (PARSE_ERROR, SKIPPED, ERROR)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Accuracy\n",
        "accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
        "\n",
        "# Precision / Recall / F1 (Macro)\n",
        "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "    y_true_filtered,\n",
        "    y_pred_filtered,\n",
        "    average=\"macro\"\n",
        ")\n",
        "\n",
        "# Weighted F1\n",
        "f1_weighted = f1_score(\n",
        "    y_true_filtered,\n",
        "    y_pred_filtered,\n",
        "    average=\"weighted\"\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EVALUATION METRICS (COT Model on HuggingFace Test Set)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nAccuracy        : {accuracy:.4f}\")\n",
        "print(f\"Precision (Mac) : {precision_macro:.4f}\")\n",
        "print(f\"Recall (Mac)    : {recall_macro:.4f}\")\n",
        "print(f\"F1 (Macro)      : {f1_macro:.4f}\")\n",
        "print(f\"F1 (Weighted)   : {f1_weighted:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "print(classification_report(\n",
        "    y_true_filtered,\n",
        "    y_pred_filtered,\n",
        "    digits=4\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nðŸ“Š Prediction vs Ground Truth Distribution:\")\n",
        "print(\"\\nGround Truth:\")\n",
        "for label in [\"Clear Reply\", \"Clear Non-Reply\", \"Ambivalent\"]:\n",
        "    count = (y_true_filtered == label).sum()\n",
        "    print(f\"   {label}: {count}\")\n",
        "\n",
        "print(\"\\nPredictions:\")\n",
        "for label in [\"Clear Reply\", \"Clear Non-Reply\", \"Ambivalent\"]:\n",
        "    count = (y_pred_filtered == label).sum()\n",
        "    print(f\"   {label}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = [\"Clear Reply\", \"Clear Non-Reply\", \"Ambivalent\"]\n",
        "cm = confusion_matrix(y_true_filtered, y_pred_filtered, labels=labels)\n",
        "\n",
        "print(\"\\nðŸ“ˆ Confusion Matrix:\")\n",
        "print(f\"{'':20} {'Pred CR':>12} {'Pred CNR':>12} {'Pred Amb':>12}\")\n",
        "print(\"-\" * 60)\n",
        "for i, true_label in enumerate(labels):\n",
        "    row = f\"{true_label:20}\"\n",
        "    for j in range(3):\n",
        "        row += f\" {cm[i][j]:>12}\"\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# View Results DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"CSV Results:\")\n",
        "csv_results_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"HuggingFace Test Results:\")\n",
        "hf_df.head(10)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
