{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Nemotron-3 Nano 30B Fine-tuning on Clarity Dataset\n",
        "\n",
        "This notebook fine-tunes the Nemotron-3 Nano 30B model on the Frenzyknight/clarity-dataset for political discourse clarity classification.\n",
        "\n",
        "**Task**: 3-class classification (Clear Reply, Ambivalent, Clear Non-Reply)\n",
        "\n",
        "Run this on an A100 GPU (e.g., Google Colab Pro or similar)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, importlib.util\n",
        "!pip install --upgrade -qqq uv\n",
        "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):\n",
        "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
        "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
        "    !uv pip install -qqq \\\n",
        "        \"torch==2.7.1\" \"triton>=3.3.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
        "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
        "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\"\n",
        "elif importlib.util.find_spec(\"unsloth\") is None:\n",
        "    !uv pip install -qqq unsloth\n",
        "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo\n",
        "\n",
        "# These are mamba kernels and we must have these for faster training\n",
        "# Mamba kernels are for now supported only on torch==2.7.1. If you have newer torch versions, please wait 30 minutes for it to compile\n",
        "!uv pip install --no-build-isolation mamba_ssm==2.2.5\n",
        "!uv pip install --no-build-isolation causal_conv1d==1.5.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Nemotron Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Nemotron-3-Nano-30B-A3B\",\n",
        "    max_seq_length = 2048,  # Sufficient for clarity classification task\n",
        "    load_in_4bit = False,   # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False,   # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False,# [NEW!] We have full finetuning now!\n",
        "    trust_remote_code = True,\n",
        "    unsloth_force_compile = True,\n",
        "    attn_implementation=\"eager\",\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add LoRA Adapters\n",
        "\n",
        "We add LoRA adapters so we only need to update a small amount of parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,  # Rank - higher for more capacity. Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "                      \"in_proj\", \"out_proj\",],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0,  # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",     # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\",  # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,   # We support rank stabilized LoRA\n",
        "    loftq_config = None,  # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Clarity Dataset\n",
        "\n",
        "We load the Frenzyknight/clarity-dataset which contains political interview Q&A pairs labeled for clarity classification.\n",
        "\n",
        "The dataset has a `conversations` column with the format:\n",
        "```\n",
        "[\n",
        "    {\"role\": \"system\", \"content\": \"...\"},\n",
        "    {\"role\": \"user\", \"content\": \"...\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Clear Reply\" | \"Ambivalent\" | \"Clear Non-Reply\"}\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the clarity dataset from HuggingFace\n",
        "dataset = load_dataset(\"Frenzyknight/clarity-dataset\", split=\"train\")\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)} examples\")\n",
        "print(f\"Columns: {dataset.column_names}\")\n",
        "print(f\"\\nSample conversation:\")\n",
        "print(dataset[0]['conversations'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply Chat Template\n",
        "\n",
        "We apply the Nemotron chat template to the conversations and save to the `text` field.\n",
        "\n",
        "Nemotron uses the following format:\n",
        "```\n",
        "<|im_start|>system\n",
        "...<|im_end|>\n",
        "<|im_start|>user\n",
        "...<|im_end|>\n",
        "<|im_start|>assistant\n",
        "...<|im_end|>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [\n",
        "        tokenizer.apply_chat_template(\n",
        "            convo, \n",
        "            tokenize=False, \n",
        "            add_generation_prompt=False\n",
        "        ) \n",
        "        for convo in convos\n",
        "    ]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's see how the chat template formatted our data\n",
        "print(\"Sample formatted text:\")\n",
        "print(\"=\"*80)\n",
        "print(dataset[0]['text'][:2000])  # Print first 2000 chars\n",
        "print(\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Training\n",
        "\n",
        "We use the SFTTrainer from TRL with optimized settings for the clarity classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    eval_dataset = None,  # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 2,  # Effective batch size = 4 * 2 = 8\n",
        "        warmup_steps = 10,\n",
        "        num_train_epochs = 3,  # Train for 3 epochs on clarity dataset\n",
        "        # max_steps = 100,  # Uncomment for quick test run\n",
        "        learning_rate = 2e-4,  # Standard for LoRA fine-tuning\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",  # Use \"wandb\" for Weights & Biases logging\n",
        "        save_strategy = \"epoch\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train on Responses Only\n",
        "\n",
        "We use Unsloth's `train_on_responses_only` to only train on the assistant outputs (the classification labels) and ignore the loss on the user's inputs. This helps increase accuracy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|im_start|>user\\n\",\n",
        "    response_part = \"<|im_start|>assistant\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify masking is working - the input should be masked (shown as spaces)\n",
        "print(\"Full input:\")\n",
        "print(tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])[:1000])\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nMasked labels (only assistant response should be visible):\")\n",
        "labels = trainer.train_dataset[0][\"labels\"]\n",
        "decoded = tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in labels])\n",
        "print(decoded.replace(tokenizer.pad_token, \" \")[-500:])  # Show end where label is"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check GPU Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show final memory stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_training = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "training_percentage = round(used_memory_for_training / max_memory * 100, 3)\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_training} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage}%.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {training_percentage}%.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference - Test the Model\n",
        "\n",
        "Let's test our fine-tuned model on a sample from the clarity task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Sample test prompt\n",
        "test_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are an expert political discourse analyst specializing in classifying response clarity in political interviews. Your task is to determine whether a politician's response to a specific question is a Clear Reply, Clear Non-Reply, or Ambivalent.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Based on a segment of the interview where the interviewer asks a series of questions, classify the type of response provided by the interviewee for the following question.\n",
        "\n",
        "### Classification Categories ###\n",
        "1. Clear Reply - The information requested is explicitly stated (in the requested form)\n",
        "2. Clear Non-Reply - The information requested is not given at all due to ignorance, need for clarification, or declining to answer\n",
        "3. Ambivalent - The information requested is given in an incomplete way (e.g., the answer is too general, partial, implicit, dodging, or deflection)\n",
        "\n",
        "### Full Interview Question ###\n",
        "Do you support the new healthcare bill?\n",
        "\n",
        "### Full Interview Answer ###\n",
        "Well, healthcare is certainly important to all Americans. We need to make sure that everyone has access to quality care. There are many aspects of this bill that we're still reviewing.\n",
        "\n",
        "### Specific Question to Classify ###\n",
        "Do you support the new healthcare bill?\n",
        "\n",
        "Classify the response clarity for this specific question. Respond with only one of: Clear Reply, Clear Non-Reply, or Ambivalent\"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    test_messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids = inputs,\n",
        "    max_new_tokens = 64,\n",
        "    use_cache = True,\n",
        "    temperature = 0.1,\n",
        "    do_sample = True,\n",
        ")\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Model Response:\")\n",
        "print(\"=\"*80)\n",
        "# Extract just the assistant's response\n",
        "if \"assistant\" in response:\n",
        "    print(response.split(\"assistant\")[-1].strip())\n",
        "else:\n",
        "    print(response[-200:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save the Model\n",
        "\n",
        "Save the LoRA adapters locally and optionally push to HuggingFace Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save locally\n",
        "model.save_pretrained(\"nemotron_clarity_lora\")\n",
        "tokenizer.save_pretrained(\"nemotron_clarity_lora\")\n",
        "print(\"Model saved to nemotron_clarity_lora/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Push to HuggingFace Hub\n",
        "# Uncomment and fill in your details to push\n",
        "\n",
        "# model.push_to_hub(\n",
        "#     \"your-username/nemotron-clarity-lora\",\n",
        "#     token = \"hf_...\",  # Your HuggingFace token\n",
        "# )\n",
        "# tokenizer.push_to_hub(\n",
        "#     \"your-username/nemotron-clarity-lora\",\n",
        "#     token = \"hf_...\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save as Merged Model (Optional)\n",
        "\n",
        "If you want to save the full merged model (base + LoRA) for easier deployment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save merged 16-bit model (larger but easier to use)\n",
        "# model.save_pretrained_merged(\"nemotron_clarity_merged\", tokenizer, save_method=\"merged_16bit\")\n",
        "\n",
        "# Or save as GGUF for llama.cpp\n",
        "# model.save_pretrained_gguf(\"nemotron_clarity_gguf\", tokenizer, quantization_method=\"q4_k_m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Full Test Set Inference & Evaluation\n",
        "\n",
        "Run inference on the complete test dataset, compute metrics, and save results to CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test set\n",
        "test_dataset = load_dataset(\"Frenzyknight/clarity-dataset\", split=\"test\")\n",
        "print(f\"Test set size: {len(test_dataset)} examples\")\n",
        "print(f\"Columns: {test_dataset.column_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# Label normalization function\n",
        "def normalize_label(text):\n",
        "    \"\"\"Normalize model output to standard labels.\"\"\"\n",
        "    if not text:\n",
        "        return \"Unknown\"\n",
        "    \n",
        "    text_lower = text.lower().strip()\n",
        "    \n",
        "    # Check for exact matches first\n",
        "    if \"clear reply\" in text_lower and \"non\" not in text_lower:\n",
        "        return \"Clear Reply\"\n",
        "    elif \"clear non-reply\" in text_lower or \"clear non reply\" in text_lower:\n",
        "        return \"Clear Non-Reply\"\n",
        "    elif \"ambivalent\" in text_lower:\n",
        "        return \"Ambivalent\"\n",
        "    \n",
        "    # Fallback patterns\n",
        "    if text_lower.startswith(\"clear reply\"):\n",
        "        return \"Clear Reply\"\n",
        "    elif text_lower.startswith(\"clear non\"):\n",
        "        return \"Clear Non-Reply\"\n",
        "    elif text_lower.startswith(\"ambivalent\"):\n",
        "        return \"Ambivalent\"\n",
        "    \n",
        "    return \"Unknown\"\n",
        "\n",
        "def extract_prediction(response_text):\n",
        "    \"\"\"Extract the prediction from model response.\"\"\"\n",
        "    # Try to find assistant response\n",
        "    if \"<|im_start|>assistant\" in response_text:\n",
        "        pred = response_text.split(\"<|im_start|>assistant\")[-1]\n",
        "        pred = pred.replace(\"<|im_end|>\", \"\").strip()\n",
        "    elif \"assistant\" in response_text.lower():\n",
        "        pred = response_text.split(\"assistant\")[-1].strip()\n",
        "    else:\n",
        "        # Take the last part of the response\n",
        "        pred = response_text.strip()\n",
        "    \n",
        "    # Clean up\n",
        "    pred = pred.strip().split(\"\\n\")[0]  # Take first line only\n",
        "    return pred\n",
        "\n",
        "print(\"Label normalization test:\")\n",
        "print(f\"  'Clear Reply' -> {normalize_label('Clear Reply')}\")\n",
        "print(f\"  'clear non-reply' -> {normalize_label('clear non-reply')}\")\n",
        "print(f\"  'Ambivalent.' -> {normalize_label('Ambivalent.')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run inference on full test set\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "results = []\n",
        "\n",
        "for idx, example in enumerate(tqdm(test_dataset, desc=\"Running inference\")):\n",
        "    convo = example['conversations']\n",
        "    \n",
        "    # Extract ground truth label (assistant response)\n",
        "    ground_truth = None\n",
        "    for msg in convo:\n",
        "        if msg['role'] == 'assistant':\n",
        "            ground_truth = msg['content']\n",
        "            break\n",
        "    \n",
        "    # Prepare inference conversation (without assistant response)\n",
        "    inference_convo = [msg for msg in convo if msg['role'] != 'assistant']\n",
        "    \n",
        "    # Tokenize and generate\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        inference_convo,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=32,\n",
        "            use_cache=True,\n",
        "            temperature=0.1,\n",
        "            do_sample=False,  # Greedy decoding for consistency\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    \n",
        "    # Decode response\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    raw_prediction = extract_prediction(full_response)\n",
        "    normalized_prediction = normalize_label(raw_prediction)\n",
        "    normalized_ground_truth = normalize_label(ground_truth) if ground_truth else \"Unknown\"\n",
        "    \n",
        "    # Extract user prompt for context\n",
        "    user_content = \"\"\n",
        "    for msg in convo:\n",
        "        if msg['role'] == 'user':\n",
        "            user_content = msg['content'][:500]  # Truncate for CSV\n",
        "            break\n",
        "    \n",
        "    results.append({\n",
        "        'idx': idx,\n",
        "        'ground_truth': ground_truth,\n",
        "        'ground_truth_normalized': normalized_ground_truth,\n",
        "        'raw_prediction': raw_prediction,\n",
        "        'prediction': normalized_prediction,\n",
        "        'correct': normalized_prediction == normalized_ground_truth,\n",
        "        'user_prompt': user_content,\n",
        "    })\n",
        "    \n",
        "    # Progress update every 50 examples\n",
        "    if (idx + 1) % 50 == 0:\n",
        "        current_acc = sum(r['correct'] for r in results) / len(results)\n",
        "        print(f\"  Progress: {idx+1}/{len(test_dataset)} | Current Accuracy: {current_acc:.2%}\")\n",
        "\n",
        "print(f\"\\nInference complete! Processed {len(results)} examples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display sample results\n",
        "print(\"Sample Results:\")\n",
        "print(\"=\"*80)\n",
        "display(results_df[['idx', 'ground_truth_normalized', 'prediction', 'correct', 'raw_prediction']].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Filter out unknown predictions for metrics\n",
        "valid_results = results_df[\n",
        "    (results_df['prediction'] != 'Unknown') & \n",
        "    (results_df['ground_truth_normalized'] != 'Unknown')\n",
        "].copy()\n",
        "\n",
        "print(f\"Total examples: {len(results_df)}\")\n",
        "print(f\"Valid examples (excluding Unknown): {len(valid_results)}\")\n",
        "print(f\"Unknown predictions: {len(results_df) - len(valid_results)}\")\n",
        "\n",
        "# Calculate accuracy\n",
        "if len(valid_results) > 0:\n",
        "    y_true = valid_results['ground_truth_normalized'].tolist()\n",
        "    y_pred = valid_results['prediction'].tolist()\n",
        "    \n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"ACCURACY: {accuracy:.2%} ({sum(valid_results['correct'])}/{len(valid_results)})\")\n",
        "    print(f\"{'='*50}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification Report\n",
        "if len(valid_results) > 0:\n",
        "    labels = ['Clear Reply', 'Ambivalent', 'Clear Non-Reply']\n",
        "    \n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(\"=\"*60)\n",
        "    print(classification_report(y_true, y_pred, labels=labels, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "if len(valid_results) > 0:\n",
        "    labels = ['Clear Reply', 'Ambivalent', 'Clear Non-Reply']\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    \n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"{'':20} {'Predicted':^45}\")\n",
        "    print(f\"{'':20} {'Clear Reply':^15} {'Ambivalent':^15} {'Clear Non-Reply':^15}\")\n",
        "    print(\"-\"*65)\n",
        "    for i, label in enumerate(labels):\n",
        "        print(f\"{label:20} {cm[i][0]:^15} {cm[i][1]:^15} {cm[i][2]:^15}\")\n",
        "    \n",
        "    # Also show as heatmap if matplotlib available\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "        \n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=labels, yticklabels=labels)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.title('Confusion Matrix - Nemotron Clarity Classification')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except ImportError:\n",
        "        print(\"\\n(Install matplotlib and seaborn for confusion matrix visualization)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-class accuracy breakdown\n",
        "if len(valid_results) > 0:\n",
        "    print(\"\\nPer-Class Accuracy:\")\n",
        "    print(\"=\"*50)\n",
        "    for label in labels:\n",
        "        class_df = valid_results[valid_results['ground_truth_normalized'] == label]\n",
        "        if len(class_df) > 0:\n",
        "            class_acc = class_df['correct'].sum() / len(class_df)\n",
        "            print(f\"  {label:20}: {class_acc:.2%} ({class_df['correct'].sum()}/{len(class_df)})\")\n",
        "        else:\n",
        "            print(f\"  {label:20}: N/A (no examples)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show prediction distribution\n",
        "print(\"\\nPrediction Distribution:\")\n",
        "print(\"=\"*50)\n",
        "print(results_df['prediction'].value_counts())\n",
        "\n",
        "print(\"\\nGround Truth Distribution:\")\n",
        "print(\"=\"*50)\n",
        "print(results_df['ground_truth_normalized'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Results to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save full results\n",
        "output_csv = \"nemotron_clarity_predictions.csv\"\n",
        "results_df.to_csv(output_csv, index=False)\n",
        "print(f\"Results saved to: {output_csv}\")\n",
        "\n",
        "# Also save a summary\n",
        "summary = {\n",
        "    'total_examples': len(results_df),\n",
        "    'valid_examples': len(valid_results),\n",
        "    'accuracy': accuracy if len(valid_results) > 0 else 0,\n",
        "    'unknown_predictions': (results_df['prediction'] == 'Unknown').sum(),\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"EVALUATION SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "for k, v in summary.items():\n",
        "    if isinstance(v, float):\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "    else:\n",
        "        print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show some incorrect predictions for analysis\n",
        "incorrect = results_df[~results_df['correct']].head(10)\n",
        "if len(incorrect) > 0:\n",
        "    print(\"\\nSample Incorrect Predictions (for analysis):\")\n",
        "    print(\"=\"*80)\n",
        "    for _, row in incorrect.iterrows():\n",
        "        print(f\"\\nExample {row['idx']}:\")\n",
        "        print(f\"  Ground Truth: {row['ground_truth_normalized']}\")\n",
        "        print(f\"  Predicted:    {row['prediction']}\")\n",
        "        print(f\"  Raw Output:   {row['raw_prediction'][:100]}...\")\n",
        "        print(\"-\"*40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download Results (Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download CSV (for Google Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(output_csv)\n",
        "    print(f\"Downloaded: {output_csv}\")\n",
        "except:\n",
        "    print(f\"CSV saved locally: {output_csv}\")\n",
        "    print(\"(Run in Colab to auto-download)\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
