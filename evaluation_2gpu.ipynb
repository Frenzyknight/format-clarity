{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clarity Model Evaluation (2-GPU)\n",
        "Loads the model distributed across 2 GPUs using FP16 and runs inference on the HuggingFace test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies if needed\n",
        "%pip install transformers accelerate datasets scikit-learn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "import warnings\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, \n",
        "    precision_recall_fscore_support, \n",
        "    f1_score, \n",
        "    classification_report, \n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "# Suppress attention mask warning\n",
        "warnings.filterwarnings(\"ignore\", message=\".*attention_mask.*\")\n",
        "\n",
        "# Check available GPUs\n",
        "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
        "for i in range(torch.cuda.device_count()):\n",
        "    props = torch.cuda.get_device_properties(i)\n",
        "    print(f\"  GPU {i}: {props.name} ({props.total_memory / 1024**3:.1f} GB)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== CONFIGURATION ====================\n",
        "# Change these as needed\n",
        "\n",
        "MODEL_NAME = \"Frenzyknight/Clarity-llama-70b\"  # Your model on HuggingFace\n",
        "DATASET_NAME = \"Frenzyknight/clarity-dataset\"  # Your HuggingFace dataset\n",
        "QEVASION_DATASET = \"ailsntua/QEvasion\"  # For ground truth labels\n",
        "\n",
        "OUTPUT_PATH = \"evaluation_2gpu_results.csv\"  # Output file\n",
        "MAX_NEW_TOKENS = 600  # Max tokens to generate\n",
        "TEMPERATURE = 0.1  # Generation temperature\n",
        "MAX_INPUT_LENGTH = 3500  # Max input tokens\n",
        "LIMIT = None  # Set to a number for testing (e.g., 10), None for full run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== HELPER FUNCTIONS ====================\n",
        "def extract_label(response: str) -> str:\n",
        "    \"\"\"Extract label from COT response.\"\"\"\n",
        "    # Look for explicit LABEL: pattern\n",
        "    match = re.search(r'LABEL:\\s*(Clear Reply|Clear Non-Reply|Ambivalent)', response, re.IGNORECASE)\n",
        "    if match:\n",
        "        label = match.group(1).strip().lower()\n",
        "        if \"non-reply\" in label:\n",
        "            return \"Clear Non-Reply\"\n",
        "        elif \"clear reply\" in label:\n",
        "            return \"Clear Reply\"\n",
        "        elif \"ambivalent\" in label:\n",
        "            return \"Ambivalent\"\n",
        "    \n",
        "    # Fallback: check last 100 chars\n",
        "    last_part = response[-100:].lower() if len(response) > 100 else response.lower()\n",
        "    if \"clear non-reply\" in last_part or \"non-reply\" in last_part:\n",
        "        return \"Clear Non-Reply\"\n",
        "    elif \"clear reply\" in last_part:\n",
        "        return \"Clear Reply\"\n",
        "    elif \"ambivalent\" in last_part:\n",
        "        return \"Ambivalent\"\n",
        "    \n",
        "    return \"PARSE_ERROR\"\n",
        "\n",
        "\n",
        "print(\"âœ… Helper functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== LOAD MODEL (2 GPUs) ====================\n",
        "print(\"=\" * 60)\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "print(\"Distributing across available GPUs with FP16...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"\\nLoading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model distributed across GPUs\n",
        "print(\"Loading model with device_map='auto' (distributes across all GPUs)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",  # Automatically distribute layers across GPUs\n",
        "    torch_dtype=torch.float16,  # FP16 for efficiency\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Model loaded successfully!\")\n",
        "if hasattr(model, 'hf_device_map'):\n",
        "    # Show how model is distributed\n",
        "    devices = set(model.hf_device_map.values())\n",
        "    print(f\"Model distributed across devices: {devices}\")\n",
        "    # Count layers per device\n",
        "    device_counts = {}\n",
        "    for layer, device in model.hf_device_map.items():\n",
        "        device_counts[device] = device_counts.get(device, 0) + 1\n",
        "    for device, count in sorted(device_counts.items()):\n",
        "        print(f\"  Device {device}: {count} modules\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== LOAD TEST DATASET ====================\n",
        "print(f\"Loading test dataset from: {DATASET_NAME}\")\n",
        "test_dataset = load_dataset(DATASET_NAME, split=\"test\")\n",
        "print(f\"âœ… Loaded {len(test_dataset)} test examples\")\n",
        "print(f\"   Columns: {test_dataset.column_names}\")\n",
        "\n",
        "# Load ground truth labels from QEvasion\n",
        "print(f\"\\nLoading ground truth from: {QEVASION_DATASET}\")\n",
        "qevasion_test = load_dataset(QEVASION_DATASET, split=\"test\")\n",
        "print(f\"âœ… Loaded {len(qevasion_test)} examples with labels\")\n",
        "\n",
        "# Apply limit if set\n",
        "if LIMIT:\n",
        "    print(f\"\\nâš ï¸  Limiting to first {LIMIT} examples (for testing)\")\n",
        "    test_dataset = test_dataset.select(range(min(LIMIT, len(test_dataset))))\n",
        "    qevasion_test = qevasion_test.select(range(min(LIMIT, len(qevasion_test))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== RUN INFERENCE ====================\n",
        "results = []\n",
        "total = len(test_dataset)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"Starting inference on {total} examples\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "for idx, example in tqdm(enumerate(test_dataset), total=total, desc=\"Evaluating\"):\n",
        "    # Get conversations from dataset (already formatted as system + user messages)\n",
        "    messages = example[\"conversations\"]\n",
        "    \n",
        "    # Format messages to string, then tokenize\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "    \n",
        "    # Tokenize the prompt string\n",
        "    encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = encoded[\"input_ids\"]\n",
        "    input_len = inputs.shape[-1]\n",
        "    \n",
        "    # Skip if too long\n",
        "    if input_len > MAX_INPUT_LENGTH:\n",
        "        tqdm.write(f\"[{idx + 1}/{total}] SKIPPED (input too long: {input_len} tokens)\")\n",
        "        results.append({\n",
        "            \"idx\": idx,\n",
        "            \"prediction\": \"SKIPPED\",\n",
        "            \"raw_output\": f\"Input too long: {input_len} tokens\"\n",
        "        })\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        # Move to model's device\n",
        "        inputs = inputs.to(model.device)\n",
        "        attention_mask = torch.ones_like(inputs)\n",
        "        \n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=inputs,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=MAX_NEW_TOKENS,\n",
        "                temperature=TEMPERATURE,\n",
        "                do_sample=TEMPERATURE > 0,\n",
        "                use_cache=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "        \n",
        "        # Decode only the new tokens\n",
        "        full_response = tokenizer.decode(\n",
        "            outputs[0, input_len:],\n",
        "            skip_special_tokens=True\n",
        "        ).strip()\n",
        "        \n",
        "        # Extract label\n",
        "        prediction = extract_label(full_response)\n",
        "        \n",
        "        results.append({\n",
        "            \"idx\": idx,\n",
        "            \"prediction\": prediction,\n",
        "            \"raw_output\": full_response\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        tqdm.write(f\"[{idx + 1}/{total}] ERROR: {e}\")\n",
        "        results.append({\n",
        "            \"idx\": idx,\n",
        "            \"prediction\": \"ERROR\",\n",
        "            \"raw_output\": str(e)\n",
        "        })\n",
        "    \n",
        "    # Clear CUDA cache periodically\n",
        "    if (idx + 1) % 25 == 0:\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nâœ… Inference complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== CREATE RESULTS DATAFRAME ====================\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Add ground truth labels\n",
        "results_df[\"clarity_label\"] = [qevasion_test[i][\"clarity_label\"] for i in range(len(results_df))]\n",
        "results_df[\"annotator1\"] = [qevasion_test[i][\"annotator1\"] for i in range(len(results_df))]\n",
        "results_df[\"annotator2\"] = [qevasion_test[i][\"annotator2\"] for i in range(len(results_df))]\n",
        "results_df[\"annotator3\"] = [qevasion_test[i][\"annotator3\"] for i in range(len(results_df))]\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv(OUTPUT_PATH, index=False)\n",
        "print(f\"âœ… Saved results to {OUTPUT_PATH}\")\n",
        "print(f\"\\nDataFrame shape: {results_df.shape}\")\n",
        "results_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== FILTER VALID PREDICTIONS ====================\n",
        "y_true = results_df[\"clarity_label\"].str.strip()\n",
        "y_pred = results_df[\"prediction\"].str.strip()\n",
        "\n",
        "VALID_LABELS = {\"Clear Reply\", \"Clear Non-Reply\", \"Ambivalent\"}\n",
        "mask = y_true.isin(VALID_LABELS) & y_pred.isin(VALID_LABELS)\n",
        "\n",
        "y_true_filtered = y_true[mask]\n",
        "y_pred_filtered = y_pred[mask]\n",
        "\n",
        "print(f\"Valid predictions: {mask.sum()} / {len(mask)}\")\n",
        "print(f\"Filtered out: {(~mask).sum()} (PARSE_ERROR, SKIPPED, ERROR)\")\n",
        "\n",
        "# Show breakdown of filtered\n",
        "invalid_preds = y_pred[~mask].value_counts()\n",
        "if len(invalid_preds) > 0:\n",
        "    print(\"\\nInvalid prediction breakdown:\")\n",
        "    for label, count in invalid_preds.items():\n",
        "        print(f\"  {label}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== COMPUTE METRICS ====================\n",
        "accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
        "\n",
        "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "    y_true_filtered,\n",
        "    y_pred_filtered,\n",
        "    average=\"macro\"\n",
        ")\n",
        "\n",
        "f1_weighted = f1_score(\n",
        "    y_true_filtered,\n",
        "    y_pred_filtered,\n",
        "    average=\"weighted\"\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EVALUATION METRICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nAccuracy        : {accuracy:.4f}\")\n",
        "print(f\"Precision (Mac) : {precision_macro:.4f}\")\n",
        "print(f\"Recall (Mac)    : {recall_macro:.4f}\")\n",
        "print(f\"F1 (Macro)      : {f1_macro:.4f}\")\n",
        "print(f\"F1 (Weighted)   : {f1_weighted:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== CLASSIFICATION REPORT ====================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "print(classification_report(\n",
        "    y_true_filtered,\n",
        "    y_pred_filtered,\n",
        "    digits=4\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== CONFUSION MATRIX ====================\n",
        "labels = [\"Clear Reply\", \"Clear Non-Reply\", \"Ambivalent\"]\n",
        "cm = confusion_matrix(y_true_filtered, y_pred_filtered, labels=labels)\n",
        "\n",
        "print(\"\\nðŸ“ˆ Confusion Matrix:\")\n",
        "print(f\"{'':20} {'Pred CR':>12} {'Pred CNR':>12} {'Pred Amb':>12}\")\n",
        "print(\"-\" * 60)\n",
        "for i, true_label in enumerate(labels):\n",
        "    row = f\"{true_label:20}\"\n",
        "    for j in range(3):\n",
        "        row += f\" {cm[i][j]:>12}\"\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== DISTRIBUTION COMPARISON ====================\n",
        "print(\"\\nðŸ“Š Prediction vs Ground Truth Distribution:\")\n",
        "print(\"\\nGround Truth:\")\n",
        "for label in labels:\n",
        "    count = (y_true_filtered == label).sum()\n",
        "    pct = count / len(y_true_filtered) * 100\n",
        "    print(f\"   {label}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "print(\"\\nPredictions:\")\n",
        "for label in labels:\n",
        "    count = (y_pred_filtered == label).sum()\n",
        "    pct = count / len(y_pred_filtered) * 100\n",
        "    print(f\"   {label}: {count} ({pct:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== SAMPLE PREDICTIONS ====================\n",
        "print(\"\\nðŸ” Sample Predictions (first 10):\")\n",
        "print(\"-\" * 80)\n",
        "for i in range(min(10, len(results_df))):\n",
        "    row = results_df.iloc[i]\n",
        "    pred = row['prediction']\n",
        "    true = row['clarity_label']\n",
        "    match = \"âœ“\" if pred == true else \"âœ—\"\n",
        "    raw_preview = str(row['raw_output'])[:60] + \"...\" if len(str(row['raw_output'])) > 60 else row['raw_output']\n",
        "    print(f\"{i+1}. {match} Pred: {pred:15} | True: {true:15}\")\n",
        "    print(f\"   Raw: {raw_preview}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== CLEANUP ====================\n",
        "# Free GPU memory\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "print(\"âœ… GPU memory cleared\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
